[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Sympatheia",
    "section": "",
    "text": "AI and Jobs\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nNov 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainties in Climate Risk Modelling\n\n\n\nSustainability\n\n\nStatistics\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n‘Smart Regulations’ in the age of AI\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI an ‘existential risk’?\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI Washing - Is it time to put a brake on the AI Hype Train?\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Sustainability - How to make them coexist?\n\n\n\nAI\n\n\nSustainability\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGoodhart’s Law - When Chasing Measures Goes Wrong\n\n\n\nEconomics\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBare Beginnings\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Linkedin\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website!\nAs a sustainability professional, I enjoy connecting the dots: be it ideas from different disciplines, people from different teams, or applications from different industries. I have strong technical skills and an academic background in ESG, data analytics, and statistics.\nMy passion lies in exploring the connection between financial sector players and the link between sustainability and financial performance of enterprises. I excel when working in collaborative groups to solve real-world business problems and am eager to adapt and learn in different environments.\nWhen I am not in the office I love playing my bass guitar, pursuing my fitness goals or baking new treats! Please feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "photography/index.html",
    "href": "photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-languages-of-middle-earth",
    "href": "projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "posts/2023-12-18- Actual Post/index.html",
    "href": "posts/2023-12-18- Actual Post/index.html",
    "title": "Aryamik Sharma",
    "section": "",
    "text": "Hi There, this is for the actual post.\n\nimport numpy as np\n\n\n#importing libraries\nimport altair as alt\nfrom bokeh.sampledata.penguins import data as penguins\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#plotting\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(\n  data=penguins, \n  title=\"Palmer Penguins Dataset\",\n  ).mark_circle(size=60).encode(\n  alt.X('bill_length_mm', scale=alt.Scale(domain=[30,60])),\n  alt.Y('bill_depth_mm', scale=alt.Scale(domain=[12,22])),\n  color='species',\n  ).add_selection(\n    brush\n)\n\nbars = alt.Chart(penguins).mark_bar().encode(\n    y='island',\n    color='island',\n    x='count(island)'\n).transform_filter(\n    brush\n)\n\npoints & bars\n\n/home/aryamik/anaconda3/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning:\n\n'selection' is deprecated.\n   Use 'selection_point()' or 'selection_interval()' instead; these functions also include more helpful docstrings.\n\n/home/aryamik/anaconda3/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning:\n\n'add_selection' is deprecated. Use 'add_params' instead."
  },
  {
    "objectID": "posts/2023-12-18-Second Pst/index.html",
    "href": "posts/2023-12-18-Second Pst/index.html",
    "title": "First Post",
    "section": "",
    "text": "#importing libraries\nimport altair as alt\nfrom bokeh.sampledata.penguins import data as penguins\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#plotting\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(\n  data=penguins, \n  title=\"Palmer Penguins Dataset\",\n  ).mark_circle(size=60).encode(\n  alt.X('bill_length_mm', scale=alt.Scale(domain=[30,60])),\n  alt.Y('bill_depth_mm', scale=alt.Scale(domain=[12,22])),\n  color='species',\n  ).add_selection(\n    brush\n)\n\nbars = alt.Chart(penguins).mark_bar().encode(\n    y='island',\n    color='island',\n    x='count(island)'\n).transform_filter(\n    brush\n)\n\npoints & bars\nHi there\nHave you guys ever seen that meme?\nThat’s me\nSo you are all wondering how does a random guy hop on this blogging platform.\nEverything below this is a test to see if the website is actually working."
  },
  {
    "objectID": "posts/2023-12-18-first-post/index.html",
    "href": "posts/2023-12-18-first-post/index.html",
    "title": "Bare Beginnings",
    "section": "",
    "text": "Hey there!\nWelcome to Sympatheia, my very own personal project that I am very excited to share with everyone!\nYou might be wondering what is this project all about or what does the word Sympatheia mean in the first place?\nA couple of years ago when I was reading Meditations by Marcus Aurelius (which is easily one of my favorite books of all time and would highly recommend that you read it atleast once!), I came across this amazing excerpt:\n\nMeditate often on the interconnectedness and mutual interdependence of all things in the universe. For in a sense, all things are mutually woven together and therefore an affinity for each other - for one thing follows after another according to their tension of movement, their sympathetic stirrings, and the unity of all substance.\n\nUpon further research, I found that the Stoics had a word for it - sympatheia - a connectedness with the cosmos.\nThis concept of interconnectedness is something that deeply resonates with me as I really enjoy connecting the dots from various disciplines in my own life.\nYou could think of Sympatheia as my own personal journal where I aim to explore the interconnectedness of some of the areas that I am really passionate about such as:\n\nESG and Sustainability\nData Science\nMachine Learning and Artificial Intelligence\nPhilosophy and Ethics\nFinance and Economics\n\nAt the end of the day it is about my own journey as I try to decipher these seemingly complexy topics and they can possibly interact with each other.\nSo now that you know a little bit about what this project is all about, I hope you are as intrigued and excited as I am. Your comments, feedback, and suggestions are invaluable to me, so feel free to share them. Stay tuned for more insights and updates!"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html",
    "href": "posts/2024-06-28-JRE/index.html",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "",
    "text": "I was listening to Episode #2156 of the Joe Rogan Experience featuring Jeremie & Edouard Harris from Gladstone AI, an organization dedicated to promoting the responsible development and adoption of AI. The whole episode had interesting tidbits that I really liked but for this post, I wanted to focus on just one segment - the economic principle of ‘Goodhart’s law’. They discuss it around 56:30:\nThis led me into a rabbit hole of Goodhart’s law and what it is all about. It can be traced back to 1975 when the British economist Charles Goodhart expressed this idea in his article “Problems of Monetary Management: The U.K. Experience”. In the article Goodhart explains:\nSimply put\nGoodhart’s law was originally developed in the context of conducting monetary policy on the basis of targets. However, its application can be observed in many other areas. Here are some other applications of Goodhart’s law that I can think of:"
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "Disclaimer",
    "section": "",
    "text": "The information provided on this website reflects my personal thoughts, opinions, and experiences. It is intended for informational purposes only and should not be taken as professional advice. All information on the site is provided in good faith, however, I make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability, availability, or completeness of any information on the site.\nAll views expressed on this site are my own and do not represent the opinions of any entity with which I have been or am currently affiliated."
  },
  {
    "objectID": "posts/2024-07-01/index.html",
    "href": "posts/2024-07-01/index.html",
    "title": "Bare Beginnings",
    "section": "",
    "text": "Hey there!\nWelcome to Sympatheia, my very own personal project that I am very excited to share with everyone!\nYou might be wondering what is this project all about or what does the word Sympatheia mean in the first place?\nA couple of years ago when I was reading Meditations by Marcus Aurelius (which is easily one of my favorite books of all time and would highly recommend that you read it atleast once!), I came across this amazing excerpt:\n\nMeditate often on the interconnectedness and mutual interdependence of all things in the universe. For in a sense, all things are mutually woven together and therefore an affinity for each other - for one thing follows after another according to their tension of movement, their sympathetic stirrings, and the unity of all substance.\n\nUpon further research, I found that the Stoics had a word for it - sympatheia - a connectedness with the cosmos.\nThis concept of interconnectedness is something that deeply resonates with me as I really enjoy connecting the dots from various disciplines in my own life.\nYou could think of Sympatheia as my own personal journal where I aim to explore the interconnectedness of some of the areas that I am really passionate about such as:\n\nESG and Sustainability\nData Science\nMachine Learning and Artificial Intelligence\nPhilosophy and Ethics\nFinance and Economics\n\nAt the end of the day it is about my own journey as I try to decipher these seemingly complexy topics and how they can possibly interact with each other.\nSo now that you know a little bit about what this project is all about, I hope you are as intrigued and excited as I am. Your comments, feedback, and suggestions are invaluable to me, so feel free to share them. Stay tuned for more insights and updates!"
  },
  {
    "objectID": "Disclaimer.html",
    "href": "Disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "The information provided on this website reflects my personal thoughts, opinions, and experiences. It is intended for informational purposes only and should not be taken as professional advice. All information on the site is provided in good faith, however, I make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability, availability, or completeness of any information on the site.\nAll views expressed on this site are my own and do not represent the opinions of any entity with which I have been or am currently affiliated."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#my-fitness-journey",
    "href": "posts/2024-06-28-JRE/index.html#my-fitness-journey",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "My Fitness Journey",
    "text": "My Fitness Journey\nAfter hearing about Goodhart’s law, It got me thinking about my own fitness journey. When I was just starting out, I used to get so fixated on the number on weighing scale because in my head I rationalized it as:\nLower weight on the weighing scale = Being closer to my fitness goals.\nI became so obsessed with the number that I saw on the weighing scale everyday that I started ‘gaming’ the system. I started pulling off some shenanigans like being over zealous with my cardio, being extremely restrictive with my calories so that the next day the scale would read the number that I wanted to see. It got to a point where I lost track of my broader fitness goals and why I was doing it in the first place - to stay fit and be more healthy.\nIt wasn’t until many years later that I realized that I was doing it all wrong. It occurred me to that weight is one of the many metrics that can be used to gauge your progress. It can serve as a proxy for your fitness journey but it is not the end all be all metric that is worth obsessing over. Not only that, but relying solely on a weight loss scale for fitness progress can be misleading. Turns out many variables affect your body weight, such as:\n\nSleep\nCarbohydrate intake\nSodium intake\nFiber\nMenstrual cycle\nFood volume\nConstipation\nStress\nHydration\n\nTherefore even if you are giving 100%, the numbers on weighing scale can fluctuate easily.\nSo what does this tell? Being obsessed with chasing a particular metric (in this case the number on weighing scale) ends up telling nothing about what I wanted to measure in the first place."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#ai-goals",
    "href": "posts/2024-06-28-JRE/index.html#ai-goals",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "AI Goals",
    "text": "AI Goals\nIn Life 3.0 (which is easily one of my favorite books of all time), Max Tegmark mentions that in our quest to building Artificial General Intelligence (AGI) as we get more intelligent and powerful machines, it will become paramount to ensure that their goals are aligned with ours. He argues that figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard and continues to be an unsolved problem. For example, if I ask a self-driving car to take me to the airport as fast as possible and it takes my word for it, the next thing I know I am being chased by helicopters and I find myself covered in vomit. I could get mad at the car all day long and tell it ‘That’s not what I wanted!’, it can argue ‘That’s what you asked for.’"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#sustainability-targets-and-initiatives",
    "href": "posts/2024-06-28-JRE/index.html#sustainability-targets-and-initiatives",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Sustainability Targets and Initiatives",
    "text": "Sustainability Targets and Initiatives\nLet’s take a look at another example of where Goodhart’s law maybe applicable - sustainability targets and initiatives.\nFollowing the Paris Agreement, the Intergovernmental Panel on Climate Change published its Special Report on Global Warming of 1.5 °C (SR15) in which stated that\n\n“Reaching and sustaining net zero global anthropogenic [human-caused] CO2 emissions and declining net non-CO2 radiative forcing would halt anthropogenic global warming on multi-decadal timescales.\n\nEver since then, there has been significant growth in the number of actors pledging net zero emissions. Many standards such as the Science Based Targets initiative (SBTi) and International Sustainability Standards have emerged that interpret the net zero concept and aim to measure progress towards net zero targets. The rise of these standards have led to the (re)emergence of various policies and mechanisms such as carbon offsets.\nThe idea behind a carbon offset is that when an entity releases greenhouse gases, they can pay someone else to remove an equivalent amount of climate pollution from the atmosphere. For example, if an automotive manufacturer in a developed country that wants to claim it is reducing its emissions, the manufacturer can pay for a patch of rainforest to be protected in the Amazon that can absorb an equal amount of emissions. This in theory – cancels out - or offsets the impact of the automotive manufacturer emissions.\nOne can see the appeal for carbon offsets. For many industries, the cost of addressing their own emissions can be a huge undertaking. By directly funding offset projects, the cost of addressing climate change becomes more manageable. It sounds pretty straightforward, right?\nIt is all fun and games until you take a look at the reality.\nHow it started:\n\n\n\nSource: Marketplace.org\n\n\nHow it’s going:\n\n\n\nSource: Bloomberg.org\n\n\nCarbon offsets have been there since 1970s. So what went wrong along the way?\nKeeping aside the limitations of carbon offsets such as overstated baselines, double counting of emissions, additionality; one can see how Goodhart’s law might be at play here.\nConsider a simple example. A dairy farm wants to setup its operations in California. This means the dairy farm will have to clear 1 hectare of land to be up and running. Theoretically, the dairy farm could offset its impact by planting 1 hectare of vegetation in Amazon. Soon enough, this will prompt other dairy farms to do the same thing and before you know it hundreds of acres of lands have been cleared in California. But you might say : ‘Aren’t they making up for their impacts by offsetting their emissions in Amazon?’\nPartially yes. But 1 hectare of land in California is not the same as 1 hectare of land in Amazon. This has been one of the criticisms of carbon offsets where offset projects tend to encourage ‘business as usual’ practices without considering the ecological complexities of the different regions.\nThe organizations can claim to be carbon neutral (which on paper they might be). But in the end, are we really ‘solving’ for the underlying problem?"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#biodiversity-conservation",
    "href": "posts/2024-06-28-JRE/index.html#biodiversity-conservation",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Biodiversity Conservation",
    "text": "Biodiversity Conservation\nSimilarly, Goodhart’s law finds it way in a related area - protecting biodiversity.\nRapid declines in populations of various vulnerable species have led to their inclusion in direct measures of biodiversity such as International Union for Conservation of Nature (IUCN), Red List Index (RLI). The Index is based on the IUCN Red List of Threatened Species™, which is widely considered to be the leading assessment of the extinction risk of species. The Red List involves the application of quantitative criteria based on population size, distribution area, and rate of decline, to assign species to different categories of relative extinction risk.\nAs a result, a substantial research effort has been devoted to examining the causes of decline in these vulnerable species. This has led to increases in conservation efforts which have had some success in slowing or even reversing the observed declines. At first glance, this can viewed as a success story. However, it has also potentially undermined the use of these species as a sustainability indicator. As policy and management interventions have focused specifically on vulnerable species, with the aim of improving this indicator, abundance of these species is arguably now less representative of the general state of the natural environment than it was hitherto. Consequently, any increase in the indicator is more likely to be a measure of specific response actions than of any general improvement in the state of the environment."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#way-forward",
    "href": "posts/2024-06-28-JRE/index.html#way-forward",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Way Forward",
    "text": "Way Forward\nThese examples illustrate the central theme of Goodhart’s law, that is: once a measure is declared as important and policy aims to reduce it, the underlying correlation will be reduced.\nThis post is not meant to be a criticism of carbon offsets, sustainability standards or measures as a whole. As someone who is part of the ‘what gets measured, gets managed’ crowd, I still believe that measures still serve an important purpose.\nBut given what we know about Goodhart’s law, two key questions emerge:\n\nWhy does it happen?\nWhat can we do about it?\n\nIn the episode, Jeremie explains this happens because you end up baking in some misalignment between what you want and what the system wants. The more powerful that system becomes the the more it exploits that gap. People will tend to affect any given indicator/measure in whichever ways can be most readily achieved. As a result, indicators become decoupled from underlying process that they are supposed to indicate, and indicator values will become artificially inflated without addressing the underlying problem.\nThat takes care of the ‘why it happens’ part. Now how do we actually address it?\nBefore coming up with ways to quantify something, we should be really clear with what are we trying to solve for? Do I want to track my daily weight or do I want to be the healthiest version of myself? Do I want to offset my emissions or do I want to make sure the global ecosystems are protected?\nOnce we are clear on the problem we are trying to solve, we should then think of all the possible ways in which the given indicator/measure could be exploited/gamified. How could our measure of choice become decoupled from the underlying process?\nSometimes we won’t have clear answers to to these questions, especially when we are trying to solve a novel problem where there has been no precedence. However, considering some of the confounding variables at the onset of the process could help us ensure that our measures are aligned with what we are trying to solve for.\nFinally, systems should be put in place to prevent manipulation of the indicators and the assessments on which they are based, to ensure that the information they provide is objective and reliable. Using multiple indicator sets, including measures of pressure as well as state variables, could help reduce scope for indicator manipulation.\nIn conclusion, measures serve an important function in guiding decision-making and assessing progress toward goals. However, there is a need for caution in interpreting the information provided by any measure that are used; at best, they can only provide a partial indication of the status and trends of the process we are trying to assess, and this needs to be appreciated by the decision-makers who employ them."
  },
  {
    "objectID": "posts/2024-07-07/index.html",
    "href": "posts/2024-07-07/index.html",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "",
    "text": "I was listening to Episode #2156 of the Joe Rogan Experience featuring Jeremie & Edouard Harris from Gladstone AI, an organization dedicated to promoting the responsible development and adoption of AI. The whole episode had interesting tidbits that I really liked but for this post, I wanted to focus on just one segment - the economic principle of ‘Goodhart’s law’. They discuss it around 56:30:\nThis led me into a rabbit hole of Goodhart’s law and figuring out what it is all about. It can be traced back to 1975 when the British economist Charles Goodhart expressed this idea in his article “Problems of Monetary Management: The U.K. Experience”. In the article Goodhart explains:\nSimply put\nGoodhart’s law was originally developed in the context of conducting monetary policy on the basis of targets. However, its application can be observed in many other areas. Here are some other applications of Goodhart’s law that I can think at the top of my head:"
  },
  {
    "objectID": "posts/2024-07-07/index.html#my-fitness-journey",
    "href": "posts/2024-07-07/index.html#my-fitness-journey",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "My Fitness Journey",
    "text": "My Fitness Journey\nAfter hearing about Goodhart’s law, it got me thinking about my own fitness journey. When I was just starting out, I used to get so fixated on the number on weighing scale because in my head I rationalized it as:\nLower weight on the weighing scale = Being closer to my fitness goals\nI became so obsessed with the number that I saw on the weighing scale everyday that I started ‘gaming’ the system. I started pulling off some shenanigans like being over zealous with my cardio, being extremely restrictive with my calories so that the next day the scale would read the number that I wanted to see. It got to a point where I lost track of my broader fitness goals and why I was doing it in the first place - to stay fit and be more healthy.\nIt wasn’t until many years later that I realized that I was doing it all wrong. It occurred me to that weight is one of the many metrics that can be used to gauge your progress. It can serve as a proxy for your fitness journey but it is not the end-all metric that is worth obsessing over. Not only that, but relying solely on a weight loss scale for fitness progress can be misleading. Turns out many variables affect your body weight, such as:\n\nSleep\nCarbohydrate intake\nSodium intake\nFiber\nMenstrual cycle\nFood volume\nConstipation\nStress\nHydration\n\nTherefore even if you are giving 100%, the numbers on weighing scale can fluctuate easily."
  },
  {
    "objectID": "posts/2024-07-07/index.html#ai-goals",
    "href": "posts/2024-07-07/index.html#ai-goals",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "AI Goals",
    "text": "AI Goals\nIn Life 3.0 (which is easily one of my favorite books of all time), Max Tegmark mentions that in our quest to building Artificial General Intelligence (AGI) as we get more intelligent and powerful machines, it will become paramount to ensure that their goals are aligned with ours. He argues that figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard and continues to be an unsolved problem. For example, if I ask a self-driving car to take me to the airport as fast as possible and it literally takes my word for it, the next thing I know the car is ignoring the speed limits and all traffic signals, I am being chased by helicopters and by the time I arrive at the airport, I find myself covered in vomit. I could get mad at the car all day long and tell it ‘That’s not what I wanted!’, it can argue ‘That’s what you asked for.’"
  },
  {
    "objectID": "posts/2024-07-07/index.html#sustainability-targets-and-initiatives",
    "href": "posts/2024-07-07/index.html#sustainability-targets-and-initiatives",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Sustainability Targets and Initiatives",
    "text": "Sustainability Targets and Initiatives\nLet’s take a look at another example of where Goodhart’s law maybe applicable - sustainability targets and initiatives.\nFollowing the Paris Agreement, the Intergovernmental Panel on Climate Change published its Special Report on Global Warming of 1.5 °C (SR15) which stated that\n\n“Reaching and sustaining net zero global anthropogenic [human-caused] CO2 emissions and declining net non-CO2radiative forcing would halt anthropogenic global warming on multi-decadal timescales.\n\nEver since then, there has been significant growth in the number of actors pledging net zero emissions. Many standards such as the Science Based Targets initiative (SBTi) and International Sustainability Standards have emerged that interpret the net zero concept and aim to measure progress towards net zero targets. The rise of these standards have led to the (re)emergence of various policies and mechanisms such as carbon offsets.\nThe idea behind a carbon offset is that when an entity releases greenhouse gases, they can pay someone else to remove an equivalent amount of climate pollution from the atmosphere. For example, if an automotive manufacturer in a developed country that wants to claim it is reducing its emissions, the manufacturer can pay for a patch of rainforest to be protected in the Amazon that can absorb an equal amount of emissions. This in theory – cancels out - or offsets the impact of the automotive manufacturer’s emissions.\nOne can see the appeal for carbon offsets. For many industries, the cost of addressing their own emissions can be a huge undertaking. By directly funding offset projects, the cost of addressing climate change becomes more manageable. It sounds pretty straightforward, right?\nIt is all fun and games until you take a look at the reality.\nHow it started:\n\n\n\nSource: Marketplace.org\n\n\nHow it’s going:\n\n\n\nSource: Bloomberg.org\n\n\nCarbon offsets have been there since 1970s. So what went wrong along the way?\nKeeping aside the limitations of carbon offsets such as overstated baselines, double counting of emissions, additionality; one can see how Goodhart’s law might be at play here.\nConsider a simple example. A dairy farm wants to setup its operations in California. This means the dairy farm will have to clear 1 hectare of land to be up and running. Theoretically, the dairy farm could offset its impact by planting 1 hectare of vegetation in Amazon. Soon enough, this will prompt other dairy farms to do the same thing and before you know it hundreds of acres of lands have been cleared in California. But you might say : ‘Aren’t they making up for their impacts by offsetting their emissions in Amazon?’\nPartially yes. But 1 hectare of land in California is not the same as 1 hectare of land in Amazon. This has been one of the criticisms of carbon offsets where offset projects tend to encourage ‘business as usual’ practices without considering the ecological complexities of the different regions. As a result, organizations can claim to be carbon neutral (which on paper they might be). But in the end, are we really ‘solving’ for the underlying problem?"
  },
  {
    "objectID": "posts/2024-07-07/index.html#biodiversity-conservation",
    "href": "posts/2024-07-07/index.html#biodiversity-conservation",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Biodiversity Conservation",
    "text": "Biodiversity Conservation\nSimilarly, Goodhart’s law finds it way in a related area - protecting biodiversity.\nRapid declines in populations of various vulnerable species have led to their inclusion in direct measures of biodiversity such as International Union for Conservation of Nature (IUCN), Red List Index (RLI). The Index is based on the IUCN Red List of Threatened Species™, which is widely considered to be the leading assessment of the extinction risk of species. The Red List involves the application of quantitative criteria based on population size, distribution area, and rate of decline, to assign species to different categories of relative extinction risk.\nA substantial research effort has been devoted to examining the causes of decline in these vulnerable species using the indicators used in RLI. This has led to increases in conservation efforts which have had some success in slowing or even reversing the observed declines. At first glance, this can viewed as a success story. However, it has also potentially undermined the use of these species as a sustainability indicator. As policy and management interventions have focused specifically on vulnerable species, with the aim of improving this indicator, abundance of these species is arguably now less representative of the general state of the natural environment than it was hitherto. Consequently, any increase in the indicator is more likely to be a measure of specific response actions than of any general improvement in the state of the environment."
  },
  {
    "objectID": "posts/2024-07-07/index.html#way-forward",
    "href": "posts/2024-07-07/index.html#way-forward",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Way Forward",
    "text": "Way Forward\nThese examples illustrate the central theme of Goodhart’s law, that is\n\nOnce a measure is declared as important and policy aims to reduce it, the underlying correlation will be reduced.\n\nThis post is not meant to be a criticism of carbon offsets, sustainability standards or measures as a whole. As someone who is part of the ‘what gets measured, gets managed’ crowd, I still believe that measures serve an important purpose.\nBut given what we know about Goodhart’s law, two key questions emerge:\n\nWhy does it happen?\nWhat can we do about it?\n\nIn the episode, Jeremie explains this happens because you end up baking in some misalignment between what you want and what the system wants. The more powerful that system becomes the the more it exploits that gap. People will tend to affect any given indicator/measure in whichever ways can be most readily achieved. As a result, indicators become decoupled from underlying process that they are supposed to indicate, and indicator values will become artificially inflated without addressing the underlying problem.\nThat takes care of the ‘why’. Now how do we actually address it?\nBefore coming up with ways to quantify something, we should be really clear with what are we trying to solve for? Do I want to track my daily weight or do I want to be the healthiest version of myself? Do I want to offset my emissions or do I want to make sure the global ecosystems are protected?\nOnce we are clear on the problem we are trying to solve, we should then think of all the possible ways in which the given indicator/measure could be exploited/gamified. How could our measure of choice become decoupled from the underlying process?\nSometimes we won’t have clear answers to to these questions, especially when we are trying to solve a novel problem where there has been no precedence. However, considering some of the confounding variables at the onset of the process could help us ensure that our measures are aligned with what we are trying to solve for.\nFinally, systems should be put in place to prevent manipulation of the indicators and the assessments on which they are based, to ensure that the information they provide is objective and reliable. Using multiple indicator sets, including measures of pressure as well as state variables, could help reduce scope for indicator manipulation.\nIn conclusion, measures serve an important function in guiding decision-making and assessing progress toward goals. However, there is a need for caution in interpreting the information provided by any measure that are used; at best, they can only provide a partial indication of the status and trends of the process we are trying to assess, and this needs to be appreciated by the decision-makers who employ them."
  },
  {
    "objectID": "posts/2024-07-15/index.html",
    "href": "posts/2024-07-15/index.html",
    "title": "AI and Sustainability - How to make them coexist?",
    "section": "",
    "text": "AI and Sustainability - two of the hottest topics right now. From regulators to policymakers, businesses, research institutions, non-governmental organizations (NGOs), academia, investors, consumers, and communities at large, everyone wants to see what does AI have to offer while at the same time Earth is witnessing record temperatures.\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious and I decided to do a deep dive to explore the interplay between these two areas and address the question ‘how do we make them compatible?’\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. One also needs to factor in the additional footprint of hardware manufacturing, transportation, infrastructure overheads as well the water required for running these models.\n\n\n\nSource: Strubell et al.\n\n\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company; all these figures shouldn’t come as a shocker.\nA surge in demand for the data centers required for artificial intelligence is having an impact on Big Tech’s carbon emissions. Google recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment.\n\nMicrosoft is in the same boat as well. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for them.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient? Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was “I agree”. AI is already showing promising results in multiple areas and some of these climate solutions are just tip of the iceberg - and they are only going to get better.\nNow it becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nHowever despite all this evidence, something just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThe idea that AI benefits will offset the environmental costs is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient. But what if it is the other way round? What if the environmental costs of running these systems outweigh the benefits they have to offer?\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there is still room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption. Many worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done?More importantly can we develop these solutions before we surpass the planetary boundaries?\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameter language model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negligible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’, Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models\nDeveloping tools for accurate energy estimation\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Processing (NLP) by:\n\nIncreasing the alignment between experiments and research hypotheses\nEncouraging the release of trained models\nSetting up tracks that target efficiency\n\nThese are just two examples that highlight the good work that is being done to address this problem from both sides. This is where approaching this problem through the lens of ‘double materiality’ might be useful - where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from both sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?"
  },
  {
    "objectID": "posts/2024-07-15/index.html#context",
    "href": "posts/2024-07-15/index.html#context",
    "title": "AI and Sustainability - Can They Coexist?",
    "section": "Context",
    "text": "Context\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious to explore these two seemingly ‘incompatible’ areas and address the question ‘how do we make them compatible?’\nBesides the risks of climate change and AI, we have risks caused by the interaction of these risks i.e .risks on AI due to climate change and risks on climate change due to AI. This is a perfect example of a ‘polycrisis’, a concept that can be tracked back to 1999 when Edgar Morin and Anne Brigitte Kern used it to describe “interwoven and overlapping crises” in their book Homeland Earth: A Manifesto for a New Millennium. In theory, managing multiple risks in isolation is feasible. However it gets a lot more complicated when you have two risks interacting with each other."
  },
  {
    "objectID": "posts/2024-07-15/index.html#quick-facts",
    "href": "posts/2024-07-15/index.html#quick-facts",
    "title": "AI and Sustainability - Can They Coexist?",
    "section": "Quick Facts",
    "text": "Quick Facts\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. Not only that these AI models can have an impact on water use.\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company, all these figures shouldn’t come as a shocker.\nGoogle recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, andthe emissions associated with the expected increases in our technical infrastructure investment.\n\nFor Microsoft, it is a similar theme. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report published Wednesday.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for the Seattle-based tech giant.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient. Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was ‘’I agree’’. AI is already showing promising results in multiple areas and is even outperforming humans in climate solutions. These applications are just tip of the iceberg. And they are only going to get better.\nIt becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nDespite all this evidence, some thing just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThis is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient.\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there still remain room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which was a breakthrough and highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used to make even better AI systems.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption.\nMany worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems. This further complicates the problem.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done More importantly can we develop these solutions before we surpass the planetary boundaries?\nDo we curb the use of AI altogether? The problem is not use of AI per se. But rather figuring out creative solutions to make them more compatible.\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameterlanguage model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negliglible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’ , Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models.\nDeveloping tools for accurate energy estimation.\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources:\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Prcoessing (NLP) by reducing the computational costs of model training. These recommendations include:\n\nIncreasing the alignment between experiments and research hypotheses.\nEncouraging the release of trained models.\nSetting up tracks that target efficiency\n\nThese are just some of recommendations that highlight the good work that is being done to make AI and Sustainability ‘compatible’ with each other.\nThis is where approaching this problem through the lens of ‘double materiality’ might be useful. Where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?\n\nThe question is not whether ChatGPT is bad\napart from figuring out the new use cases for where AI could be useful, they now also have to think about how could we mitigate some of the negative effects of large scale AI use?\nQuestion becomes ’’Can we trust AI to within narrow window of opportunity?\nPolicy making, corporate governance, community engagement, technological innovation, and international cooperation are equally vital components in our collective toolkit for addressing sustainability challenges. Each plays a crucial role in shaping how AI and other technologies are developed, regulated, and deployed to achieve long-term environmental and social goals.\nnot only could AI reduce road fatalities,\nThat’s the fundamental ethos behind ‘Utilitrainimsim’.\nhttps://www.utilitarianism.com/jsmill-utilitarianism.pdf\nSome thing just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if me using ChatGPT is indeed causing degradation of global ecosystems?’\nthought to myself and said ‘’it does make sense’‘. to an extent. A quick search of ’AI use cases in sustainability’ and you will find hundreds, if not thousands of such applications. But then I thought to myself, this assumption lies on the fact AI systems will get more efficient.\nOnce technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. The cost of information technology has now halved roughly every two years for about a century, enabling the information age.\nMy concern is not whether they are compatible or not? A more pressing concern in my opinion is how can we make them work together in a way\nSo what’s wrong? If AI is showing promise in addressing these problems are we being too paranoid about it?\nGlobal planetary boundaries. before time runs out.\nWhen we had a panel discussion about this in Austin, Texas, at the 2016 annual meeting of the Association for the Advancement of Artificial Intelligence, the Ihe exclaimed. Because almost all car crashes are caused by human error, it’s widely believed that AI-powered self-driving cars can eliminate at least 90% of road deaths, and this optimism is fueling great progress toward actually getting self-driving cars out on the roads.\nAI can assist in the identification of areas that are at high risk of climate-related hazards, the development of adaptation strategies for businesses and communities, the prediction of floods and wildfires, and the identification of areas at risk of landslides (Rutenberg et al., 2021). Furthermore, AI-powered climate modeling and projection systems can help policymakers and stakeholders anticipate the impacts of climate change and design effective mitigation and adaptation strategies. AI can also help in the development of early warning systems that can alert communities to impending disasters, providing them with crucial time to prepare and evacuate.\nthe net balance in terms of climate is that AI will be a very good\nthat extra 5% of demand is not the thing that prevents our climate goals. thing, whether it’s the material science part of the thing or managing plasmas infusion reactors, it’s going to accelerate the innovation.\nHis main argument is that\nAI will ‘pay for itself’ when it comes to its associated environmental costs. His main argument is that AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters.\nthe n\nMaybe there is less doom and gloom?\nWe all know people around the world are exploring the use cases of AI within sustainability space.\nBill Gates recently said by enabling the development of innovative strategies for climate change adaptation. AI can assist in the identification of areas that are at high risk of climate-related hazards, the development of adaptation strategies for businesses and communities, the prediction of floods and wildfires, and the identification of areas at risk of landslides (Rutenberg et al., 2021). Furthermore, AI-powered climate modeling and projection systems can help policymakers and stakeholders anticipate the impacts of climate change and design effective mitigation and adaptation strategies. AI can also help in the development of early warning systems that can alert communities to impending disasters, providing them with crucial time to prepare and evacuate.\nI can see his point totally. One of the use cases that I was excited to see was CLIMATEBERT, a transformer-based language model developed by Webersinke et al. (2021) for analysing climate-related text.\nBesides, being pleasantly surprised by the I was very pleased to see a section called ‘Carbon Footprint’\nBut turns out that didn’t result in the consumption of less electricity. On the contrary, it resulted in the same amount of electricity being used to make even better AI systems. In economics, this phenomenon is commonly referred to as ‘Jevon’s paradox’ , named after Jevon who noted that the improvement of steam engine by James Watt which allowed for much less coal to be used, instead led to a huge increase in the amount of the fossil fuel burned in England.\nThe concern here is we wouldn’t want the same thing to happen with banking our trust in AI. We are hoping that AI will help us reduce our emissions by optimizing our\nSo what’s the way forward?\nBut are we betting too much on AI to save the world for us?\nWebersinke, N., Kraus, M., Bingler, J. A., & Leippold, M. (2021). Climatebert: A pretrained language model for climate-related text. arXiv preprint arXiv:2110.12010.\nhttps://www.bloomberg.com/news/articles/2024-06-26/why-bill-gates-is-bullish-on-artificial-intelligence-and-nuclear-energy\nIt makes me wonder\nWhen you pair these figures with\nAssuming a continued annual relative growth ranging from 5.6 to 6.9%, ICT’s relative contribution would exceed 14% of the 2016-level worldwide GHGE by 2040 […]\nWhen you pair these figures with\nWhen almost makes me question - maybe reactionary claims true\nAnd as the world’s demand for such AI technology continues to grow, so does the AI industry’s energy consumption. In an environmentally hostile chain reaction, rapidly increasing computational needs will unavoidably escalate carbon costs.\nThe data, published in Amazon’s annual sustainability report on Wednesday, contrast with disclosures from rivals Microsoft Corp. and Alphabet Inc.’s Google, which are struggling to reduce their emissions amid a surge in demand for the data centers required for artificial intelligence.\nAs datasets and models become more complex, the energy needed to train and run AI models becomes enormous. This increase in energy use directly affects greenhouse gas emissions, aggravating climate change. According to OpenAI researchers, since 2012, the amount of computing power required to train cutting-edge AI models has doubled every 3.4 months. By 2040, it is expected that the emissions from the Information and Communications Technology (ICT) industry as a whole will reach 14% of the global emissions, with the majority of those emissions coming from the ICT infrastructure, particularly data centres and communication networks. These data demonstrate the urgent need to address AI’s carbon footprint and role in environmental deterioration.\nRecently, a study was conducted by researchers at the University of Massachusetts to determine how much energy is used to train certain popular large AI models. According to the results, training can produce about 626,000 pounds of carbon dioxide, or the equivalent of around 300 round-trip flights between New York and San Francisco – nearly 5 times the lifetime emissions of the average car.\nWhat is the way forward?\nIt’s one of those things where you know there is a promise but at the same is it going to be worth it? A looming thought emerges - what\nIt is a very interesting read and I would highly encourage everyone to take a look at it.\nYou have AI and Sustainability\nThere is a lot of talk\nhttps://arxiv.org/abs/1906.02243 - Training a model emits 5 times the emissions\nBloom Model - https://cifar.ca/wp-content/uploads/2023/09/Towards-Measuring-and-Mitigating-the-Environmental-Impacts-of-Large-Language-Models.pdf\nCase Study: The Challenge with Generative AI and Large Language Models The AI carbon footprint remains of high concern as AI continues to increase in use and popularity. According to Open AI researchers, since 2012, the computing power required to train cutting-edge AI models has doubled every 3.4 months. By 2040, it is expected that the emissions from the Information and Communications Technology (ICT) industry will reach 14% of the global emissions, with the majority of those emissions coming from the ICT infrastructure, particularly data centers and communications networks.”12 Training a single large language model (LLM) like ChatGPT can emit up to 626,155 tons of CO2e, which is more than the lifetime emissions of five average American cars, including fuel.13 This is because LLMs require massive amounts of data and computing power, which consume a lot of electricity from the grid, often generated by fossil fuels. The GHG emission, energy consumption, and water use of training generative AI models like large language models can be significant. For example, some estimates suggested that training a single large language model can use around 300 to 500 tons of CO2, about 60 times the average person’s annual carbon footprint. 14 This is a significant amount of energy, especially considering that many AI models are trained on large datasets and require a lot of computing power. A separate study also estimates that large data centers used to train and run these large AI models can consume 10s of billions of liters of fresh water.15 This does not even include the footprint of hardware manufacturing, transportation, and infrastructure overheads.\nBERT excerpt about AI\n’’Nevertheless, we decided to carry out this project, as we see the high potential of NLP to support action against climate change. Given our awareness of the carbon footprint of our research, we address this sensitivetopic as follows:\nI would argue its not only an for AI question as well.\nWe can’t rely on historical data for prediciting extreme events in the future.\nNot only that,\nCreative solutions will be needed. - Jevon’s Paradox\nhttps://www.theguardian.com/business/article/2024/jul/04/can-the-climate-survive-the-insatiable-energy-demands-of-the-ai-arms-race\nBill GAtes\nIEA\nChatGPT\nhttps://www.bloomberg.com/news/articles/2024-07-02/google-s-emissions-shot-up-48-over-five-years-due-to-ai\nhttps://www.bloomberg.com/news/articles/2024-06-26/bill-gates-says-ai-will-advance-green-tech-offsetting-its-emissions\ncan we really use AI for future weather impacts?\nnot really. Historical trends do not correspond to future.\nMaybe anamoly detection.\nUnlike traditional supervised learning applications, it’s not as simple as\nTalk about pros and cons from here: https://link.springer.com/article/10.1007/s43762-023-00100-2\nWe can’t use historical data to predict future climate risk\nhttps://www.popsci.com/story/environment/underestimating-extreme-weather-climate-change/\nhttps://www.science.org/doi/10.1126/sciadv.aay2368\nhttps://www.hoover.org/research/flawed-climate-models\nhttps://20965052.fs1.hubspotusercontent-na1.net/hubfs/20965052/AIs%20Impact%20on%20Our%20Sustainable%20Future%20White%20Paper%20V1.pdf\nI think its a unique challenge for both domains. For Sustainability - apart from figuring out the new use cases for where AI could be useful, they now also have to think about how could we mitigate some of the negative effects of large scale AI use?\nFor the tech folks, the question becomes how do we make better algorithms, efficiency etc.brainstorm how can we better address these issues\nI am going to borrow the concept of dual materiality and say the question\nHow does AI have an impact on sustainability and vice versa?"
  },
  {
    "objectID": "posts/2024-07-21/index.html",
    "href": "posts/2024-07-21/index.html",
    "title": "AI and Sustainability - How to make them coexist?",
    "section": "",
    "text": "AI and Sustainability - two of the hottest topics right now. From regulators to policymakers, businesses, research institutions, non-governmental organizations (NGOs), academia, investors, consumers, and communities at large, everyone wants to see what does AI have to offer while at the same time Earth is witnessing record temperatures.\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious and I decided to do a deep dive to explore the interplay between these two areas and address the question ‘how do we make them compatible?’\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. One also needs to factor in the additional footprint of hardware manufacturing, transportation, infrastructure overheads as well the water required for running these models.\n\n\n\nSource: Strubell et al.\n\n\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company; all these figures shouldn’t come as a shocker.\nA surge in demand for the data centers required for artificial intelligence is having an impact on Big Tech’s carbon emissions. Google recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment.\n\nMicrosoft is in the same boat as well. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for them.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient? Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was “I agree”. AI is already showing promising results in multiple areas and some of these climate solutions are just tip of the iceberg - and they are only going to get better.\nNow it becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nHowever despite all this evidence, something just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThe idea that AI benefits will offset the environmental costs is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient. But what if it is the other way round? What if the environmental costs of running these systems outweigh the benefits they have to offer?\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there is still room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption. Many worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done?More importantly can we develop these solutions before we surpass the planetary boundaries?\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameter language model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negligible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’, Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models\nDeveloping tools for accurate energy estimation\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Processing (NLP) by:\n\nIncreasing the alignment between experiments and research hypotheses\nEncouraging the release of trained models\nSetting up tracks that target efficiency\n\nThese are just two examples that highlight the good work that is being done to address this problem from both sides. This is where approaching this problem through the lens of ‘double materiality’ might be useful - where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from both sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?"
  },
  {
    "objectID": "posts/2024-08-04/index.html",
    "href": "posts/2024-08-04/index.html",
    "title": "AI Washing - Is it time to put a brake on the AI Hype Train?",
    "section": "",
    "text": "If you are anything like me, every day you wake up to see something new happening in the AI space. Whether its is OpenAI creating its own search engine SearchGPT or Meta releasing Llama 3.1, AI stuff is everywhere. I am not going to lie, reading about the latest and greatest happening in the AI ecosystem is really exciting for me. The other day I read about Adobe coming up with new audio editing features that removes instances of filler words such as ‘umh’. Granted it is not perfect, but it is only a matter of time before it becomes even better.\nHowever, it is getting to the point where I am finding it too much. These days it is almost impossible for me to browse my newsfeed without ever encountering the term ‘GenAI’.\nMy honest reaction whenever I see a new 'GenAI' related post on my newsfeedvia GIPHY\nCurating my newsfeed has definitely helped. However, I asked myself: \"Are we really going overboard with AI?\" Don’t get me wrong, I am not an AI skeptic in any shape or form. On the contrary, I am excited to see how it all plays out. But maybe we need to be realistic with our expectations about AI? Big Tech’s latest earnings suggest that there isn’t much revenue to show for the AI hype train. Granted that could change in the future, but as Julia Angwin describes in her article, \"some of A.I.’s greatest accomplishments seem inflated\" and it will take a while for AI to live up to its hype of becoming the most powerful technology humanity has yet invented.\nWhat could be possibly causing this? My theory is that this misalignment between the hype and the actual reality of AI comes down to the vagueness of the term ‘artificial intelligence’ itself.\nThe term artificial intelligence has been around since 1956 when John McCarthy, coined it during a conference at Dartmouth College. The proposal stated:\n\nThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\nHow can it be that the concept of AI has been around since decades and yet there is no universally agreed upon definition of ‘artificial intelligence’ ?\nThat’s because there is no definition of the term ‘intelligence’ itself. In Life 3.0, Max Tegmark defines intelligence as the ability to accomplish complex goals. And because there are many possible goals, there are many possible types of intelligence. Every now and then I keep reading about feats of intelligence in animal kingdom from ants being able to collectively solve complex tasks to dolphins being able to sense electric currents in water (electroreception) to avoid predators, find food, navigate or even communicate with one another. Clearly they are ‘intelligent’ in their own ways, it’s just that we don’t know how to measure that intelligence. Therefore, according to Tegmark, it makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ.\nThat isn’t to say there haven’t been attempts to capture the essence of AI. AI has its roots in statistics and machine learning and can be traced back to works of Alan Turing and Arthur Samuel in 20th Century. These researchers were interested in having machines learn from data and attempted to approach the problem through generalized linear models and probabilistic reasoning. Eventually, these methods evolved and led to some early developments in the broad AI space. Somewhere along the way, the boundaries between traditional statistical methods, machine learning and artificial intelligence got blurred. Fast forward to today and the meme of ‘It’s AI in the sales pitch and machine learning in the prototype’ is now a reality.\n\n\n\n\nSource: Sandserif Comics\n\n\n\n\n\nThe power of the hype. Source: Wallace Ferreira\n\n\nJokes aside, this isn’t to say all Statistics = Machine Learning = Artificial Intelligence. Joe Davison covers this in the article ‘No, Machine Learning is not just glorified Statistics’ and sums it up succinctly : ‘’reducing machine learning as a whole to nothing more than a subsidiary of statistics is quite a stretch.’’\nInstead the way to look at it is that artificial intelligence is this broad umbrella category with machine learning being a subset of artificial intelligence and deep learning in turn being a subset of machine learning which ultimately has roots in statistics.\n\n\n\nAn illustration of the position of deep learning (DL), comparing with machine learning (ML) and artificial intelligence (AI). Source: Sarker (2021)\n\n\nThe vagueness of terms like AI starts emerging when people start conflating these terms with one another. Eric Siegel, former professor at Columbia University talks about this extensively in his work. According to Siegel,\n\nMost people conceive of ML as “AI.” This is a reasonable misunderstanding. But “AI” suffers from an unrelenting, incurable case of vagueness — it is a catch-all term of art that does not consistently refer to any particular method or value proposition. Calling ML tools “AI” oversells what most ML business deployments actually do. In fact, you couldn’t overpromise more than you do when you call something “AI.” The moniker invokes the notion of artificial general intelligence (AGI), software capable of any intellectual task humans can do.\n\nHe further talks about a paradox, known as The AI Effect where either the definition of AI or the concept of intelligence is adjusted to exclude capabilities that AI systems have mastered. In other words, if everything is AI, then nothing is truly AI.\nIf you think about it, this problem is not unique to the AI space. Sustainability space continues to be afflicted by what constitutes as ‘sustainable’? No wonder there is a huge focus on addressing greenwashing risks to seperate the facts from the fiction and cut down misleading environmental claims.\nSo going back to the title of the post, is it time to borrow a page from the sustainability space and address ‘AI washing’, investments directed towards projects labeled as AI without rigorous scrutiny of their actual impact?\nDo we really need to have ‘AI-features’ in our electric toothbrushes (unless it’s going to save me a visit to my dentist every 6 months or so, then I am all ears)? Or how about an ‘AI Button’ on your mouse and your keyboard? (I mean, really?)\nNow I am not implying that these features aren’t welcome and the we all should be become AI Luddites. I would be lying if I said the thought of trying these features didn’t occur to me. But this raises some questions such as \"Do I really need to pay $100 extra for an ‘AI-powered toothbrush’ that is pretty much going to do the same thing?\" or more broadly \"Do we really need to slap the term GenAI to something that would had been routine ‘bug fixes’ or ‘feature enhancements’ six months ago?\"\nFortunately, there is work being done to address AI washing. Last year, the Federal Trade Commission (FTC) warned companies across the economy that it would be on the lookout for false AI claims in advertising, Similarly, Securities and Exchange Commission (SEC) Chair Gary Gensler has warned businesses against making exaggerations of AI’s capabilities. Earlier this year, the SEC imposed the first civil penalties on two companies, Delphia Inc and Global Predictions Inc, for misleading statements about their use of AI. In Europe, the Artificial Intelligence Act was approved in May 2024 with the goal of harmonising rules on AI.\nLove it or hate it, I still think that AI is here to stay. Ultimately, the future of AI is one of the most important conversations we need to have. Maybe our goal shouldn’t be jumping off the AI train but rather building more transparent and responsible AI systems that facilitate open and honest discourse."
  },
  {
    "objectID": "posts/2024-08-17/index.html",
    "href": "posts/2024-08-17/index.html",
    "title": "Is AI an ‘existential risk’?",
    "section": "",
    "text": "From Terminator to Westworld, almost all depictions of AI that I have seen in media follow a similar theme: a malevolent entity that aims to subjugate human beings. In fact, I am having a hard time recalling any major piece of media that isn’t set in the dystopian future and depicts humans and AI living in harmony. If you have any good recommendations of a movie, TV series, novel or any other major piece of art that depicts humans and AI having a jolly good time, do let me know.\nBut coming to the concept of AI being an existential threat. How did I stumble across this idea?\nThe answer is this comprehensive article published on 80000 hours. I would definitely recommend giving it a read because it outlines some really good points about the future of AI. In fact, risks from artificial intelligence are ranked as the number one in the world’s most pressing problems above catastrophic pandemics, nuclear weapons, power war and climate change. On March 22, 2023, thousands of tech leaders, researchers and AI experts including Elon Musk, Stuart Russell, Max Tegmark signed an open letter urging a moratorium on the development of the most powerful artificial intelligence systems. Almost two months later, a similar letter titled ‘Statement on AI Risk’ was signed by another group of world’s leading AI scientists and experts.\nA natural question to ask would be: why is this all happening of a sudden?\nThe number of AI-lobbying organizations in the U.S. spiked from 158 in 2022 to 450 in 2023. That marks a whopping 185% increase in AI lobbying with the goal of regulating AI. There have been reports suggesting that there is in fact a growing network of AI advisers, think tanks and policy groups that are influencing federal agencies on AI policy discourse. For example, a bipartisan bill was passed in the U.S. Senate setting the framework for AI legislation. It would would create a new authority that any company developing AI would have to register to and seek a license from. Similar developments are taking place around the world. According to Stanford University’s 2023 AI Index, the annual number of bills mentioning “artificial intelligence” passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.\nAt a first glance, all these developments look like a step in the right direction. In hands of bad actors, any technology like AI can be a bad thing. We are already seeing a rise in scams, deepfakes, voice cloning, misinformation and cyberattacks.\nHowever, there are many who believe that this narrative of AI being an existential risk is being used to monopolize AI in hands of select few. Andrew Ng, a globally recognized leader in AI, believes that imposing strict licensing requirements and blanket AI regulations is is based on the “bad idea that AI could make us go extinct”. In his view, this is a way for big tech to create regulatory capture to ensure that open source alternatives cannot compete. Regulatory capture is a concept where a regulatory agency enacts policies that favor the industry at the expense of the broader public interest, in this case with regulations that are too onerous or expensive for smaller businesses to meet. This has led to a growing movement to democratize AI governance. Letters such as Joint Statement on AI Safety and Openness and A Call to Protect and Open-Source AI in Europe have been signed by various scientists, policymakers, researchers and other prominent figures in the field of AI.\nSuresh Venkatasubramanian, a professor of computer science at Brown University who co-authored last year’s White House Blueprint for an AI Bill of Rights believes that the focus on cataclysmic AI risk is “speculative science fiction” that borders on “fearmongering.” He argues:\n\n“There’s a push being made that the only thing we should care about is long-term risk because ‘It’s going to take over the world, Terminator, blah blah blah,’”\n“I think it’s important to ask, what is the basis for these claims? What is the likelihood of these claims coming to pass? And how certain are we about all this?”\n\nWhen one sits down to analyse these claims, a natural question to start off would be:\n‘Is superintelligence even possible?’ The answer to that question depends on who you ask. As Nick Bostrom mentions in Superintelligence, expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. According to Stuart Armstrong and Kaj Sotala. predictions about the future development of artificial intelligence, “are as confident as they are diverse.”\nMany surveys have been conducted on this topic where AI experts have been asked about their expectations for the future of machine intelligence (I really like this article and this survey conducted by Nick Bostrom in 2012-2013) and turns out there is huge disagreement between experts about when human-level AI will be developed. Granted there are limitations when interpreting the results of these surveys, but the take home message is that there is a high degree of uncertainty about the future of AI. That does not mean there is no risk involved. But what is that risk exactly? Is it Skynet? Ultron? Or another killer robot?\nIn Life 3.0, Max Tegmark debunks the long standing narrative of AI going rogue:\n\nThe fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.\nTherefore, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.\n\n\n\n\nSource: Can we build AI without losing control over it? | Sam Harris\n\n\nThis raises the question: ’What do we do about these risks of AI and more importantly how do we make sure our goals align with that of a superintelligent AI? In the book, Max mentions that goal alignment remains to be an unsolved problem which would require its own post but broadly speaking it can be further subdivided into three sub-problems:\n\nMaking AI learn our goals\nMaking AI adopt our goals\nMaking AI retain our goals\n\nTo wrap things up, any technology including AI poses a risk. This risk stems from misalignment between our goals and the goal of AI. In my opinion, addressing that risk will require strong policy making and open discourse to facilitate responsible AI development. Clearly there is a need for some regulatory oversight to address these risks. In my last post, I mentioned about there is work being done to address ‘AI Washing’ by regulators like Federal Trade Commission, Securities and Exchange Commission. But a question needs to be asked:\n\nWhat do we want out of these regulations? Do we want to monopolize these developments in the hands of select few or do we truly want responsible AI systems?"
  },
  {
    "objectID": "posts/2024-08-24/index.html",
    "href": "posts/2024-08-24/index.html",
    "title": "Is AI an ‘existential risk’?",
    "section": "",
    "text": "From Terminator to Westworld, almost all depictions of AI that I have seen in media follow a similar theme: a malevolent entity that aims to subjugate human beings. In fact, I am having a hard time recalling any major piece of media that isn’t set in the dystopian future and depicts humans and AI living in harmony. If you have any good recommendations of a movie, TV series, novel or any other major piece of art that depicts humans and AI having a jolly good time, do let me know.\nBut coming to the concept of AI being an existential threat. How did I stumble across this idea?\nThe answer is this comprehensive article published on 80000 hours. I would definitely recommend giving it a read because it outlines some really good points about the future of AI. In fact, risks from artificial intelligence are ranked as the number one in the world’s most pressing problems above catastrophic pandemics, nuclear weapons, power war and climate change. On March 22, 2023, thousands of tech leaders, researchers and AI experts including Elon Musk, Stuart Russell, Max Tegmark signed an open letter urging a moratorium on the development of the most powerful artificial intelligence systems. Almost two months later, a similar letter titled ‘Statement on AI Risk’ was signed by another group of world’s leading AI scientists and experts.\nA natural question to ask would be: why is this all happening of a sudden?\nThe number of AI-lobbying organizations in the U.S. spiked from 158 in 2022 to 450 in 2023. That marks a whopping 185% increase in AI lobbying with the goal of regulating AI. There have been reports suggesting that there is in fact a growing network of AI advisers, think tanks and policy groups that are influencing federal agencies on AI policy discourse. For example, a bipartisan bill was passed in the U.S. Senate setting the framework for AI legislation. It would would create a new authority that any company developing AI would have to register to and seek a license from. Similar developments are taking place around the world. According to Stanford University’s 2023 AI Index, the annual number of bills mentioning “artificial intelligence” passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.\nAt a first glance, all these developments look like a step in the right direction. In hands of bad actors, any technology like AI can be a bad thing. We are already seeing a rise in scams, deepfakes, voice cloning, misinformation and cyberattacks.\nHowever, there are many who believe that this narrative of AI being an existential risk is being used to monopolize AI in hands of select few. Andrew Ng, a globally recognized leader in AI, believes that imposing strict licensing requirements and blanket AI regulations is is based on the “bad idea that AI could make us go extinct”. In his view, this is a way for big tech to create regulatory capture to ensure that open source alternatives cannot compete. Regulatory capture is a concept where a regulatory agency enacts policies that favor the industry at the expense of the broader public interest, in this case with regulations that are too onerous or expensive for smaller businesses to meet. This has led to a growing movement to democratize AI governance. Letters such as Joint Statement on AI Safety and Openness and A Call to Protect and Open-Source AI in Europe have been signed by various scientists, policymakers, researchers and other prominent figures in the field of AI.\nSuresh Venkatasubramanian, a professor of computer science at Brown University who co-authored last year’s White House Blueprint for an AI Bill of Rights believes that the focus on cataclysmic AI risk is “speculative science fiction” that borders on “fearmongering.” He argues:\n\n“There’s a push being made that the only thing we should care about is long-term risk because ‘It’s going to take over the world, Terminator, blah blah blah,’”\n“I think it’s important to ask, what is the basis for these claims? What is the likelihood of these claims coming to pass? And how certain are we about all this?”\n\nWhen one sits down to analyse these claims, a natural question to start off would be:\n‘Is superintelligence even possible?’ The answer to that question depends on who you ask. As Nick Bostrom mentions in Superintelligence, expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. According to Stuart Armstrong and Kaj Sotala. predictions about the future development of artificial intelligence, “are as confident as they are diverse.”\nMany surveys have been conducted on this topic where AI experts have been asked about their expectations for the future of machine intelligence (I really like this article and this survey conducted by Nick Bostrom in 2012-2013) and turns out there is huge disagreement between experts about when human-level AI will be developed. Granted there are limitations when interpreting the results of these surveys, but the take home message is that there is a high degree of uncertainty about the future of AI. That does not mean there is no risk involved. But what is that risk exactly? Is it Skynet? Ultron? Or another killer robot?\nIn Life 3.0, Max Tegmark debunks the long standing narrative of AI going rogue:\n\nThe fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.\nTherefore, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.\n\n\n\n\nSource: Can we build AI without losing control over it? | Sam Harris\n\n\nThis raises the question: ’What do we do about these risks of AI and more importantly how do we make sure our goals align with that of a superintelligent AI? In the book, Max mentions that goal alignment remains to be an unsolved problem which would require its own post but broadly speaking it can be further subdivided into three sub-problems:\n\nMaking AI learn our goals\nMaking AI adopt our goals\nMaking AI retain our goals\n\nTo wrap things up, any technology including AI poses a risk. This risk stems from misalignment between our goals and the goal of AI. However, we also need to be careful that we are not overselling these risks. In my last post, I talked about how ‘AI Washing’ stems from overselling the true benefits of AI. The way I see it is that the ‘AI as an existential risk’ narrative is no different; where instead of the true benefits of AI, the focus is on exaggerating the risks of AI. The path forward is also not downplaying these risks. In my opinion, we need to focus our efforts on strong policy making and open discourse to facilitate responsible AI development. Clearly there is also a need for some regulatory oversight to address these risks. But a question needs to be asked:\n\nWhat do we want out of these regulations? Do we want to monopolize these developments in the hands of select few or do we truly want responsible AI systems?"
  },
  {
    "objectID": "posts/2024-08-31/index.html",
    "href": "posts/2024-08-31/index.html",
    "title": "‘Smart Regulations’ in the age of AI",
    "section": "",
    "text": "Riddle me this:\n\nI’m the reason you can’t do that thing you crave,\nI make sure everyone follows the wave.\nYou might love me or call me a bore,\nBut without me, chaos would be at your door.\nWhat am I?\n\nIf you guessed regulations, then you are right.\nA while back, I came across this post on r/dundermifflin. Sometime later, a totally random thought came to my head: what would be the course that I took at UW that would be analogous to that meme format?\n\n\n\n\n\nFor me, it would be ENBUS 408 – Best Practices in Regulation Instrument Choice in Environmental Policy.\n(By the way if you are wondering which character in The Office had the most impact in the story given their limited screen time, I would go with Robert California. Period.)\nI took ENBUS 408 during my last term at UW and the entire premise of that course was on designing ‘smart regulations’ when it comes to addressing environmental problems. Having been involved in this space that is constantly evolving, not a day goes by that I don’t see a new ESG regulation popping up in my newsfeed. Looking back, I can definitely say now that ENBUS 408 made me appreciate the process that goes behind designing regulations.\nMy last few posts have been focused on the idea of transparency around AI governance. As I was in the process of writing them, I realized that AI and sustainability share a lot of common features. The obvious one being that they are rapidly evolving. So I asked myself: Can we borrow a page from environmental policy making and design ‘smart regulations’ for AI? That’s what compelled me to write this post.\nWhat are smart regulations in the first place?\nThis excerpt from Chapter 8 of Regulatory Theory: Foundations and applications by Peter Drahos sums up smart regulations pretty succinctly:\n\nThe term [smart regulation] refers to a form of regulatory pluralism that embraces flexible, imaginative and innovative forms of social control. In doing so, it harnesses governments as well as business and third parties. For example, it encompasses self-regulation and co-regulation, using commercial interests and non-governmental organisations (NGOs) (such as peak bodies) as regulatory surrogates, together with improving the effectiveness and efficiency of more conventional forms of direct government regulation. The underlying rationale is that, in the majority of circumstances, the use of multiple rather than single policy instruments, and a broader range of regulatory actors, will produce better regulation. As such, it envisages the implementation of complementary combinations of instruments and participants tailored to meet the imperatives of specific environmental issues.\n\nIn the article, the authors Neil Gunningham and Darren Sinclair argue that regulations were traditionally considered a bipartiate process involving the state (the regulator) and the business (the regulated entity). However, work by Joseph Rees in 1988 highlighted that there are multiple actors influencing the behavior or regulated groups in a variety of complex and subtle ways. In an age when we are dealing with complex problems such as climate change and AI, smart regulations have emerged as an alternative where traditional command-and-control regulation nor the free market have been sufficient in addressing them. As a result, the focus of smart regulation is on understanding the dynamic interplay of different actors such as international standards organisations; trading partners and the supply chain; commercial institutions and financial markets; peer pressure and self-regulation through industry associations; and civil society.\nThe authors proposes the following ‘regulatory design principles’ that should guide these smart regulations:\n\nThe desirability of preferring complementary instrument mixes over single instrument approaches.\nThe virtues of parsimony where less interventionist measures should be preferred.\nThe benefits of an escalating response up an instrument pyramid (utilizing not only government, but also business and third parties) to build in regulatory responsiveness, to increase dependability of outcomes through instrument sequencing and to provide early warning of instrument failure through the use of triggers.\nEmpowering third parties (both commercial and non-commercial) to act as surrogate regulators, thereby achieving not only better environmental outcomes at less cost but also freeing up scarce regulatory resources, which can be redeployed in circumstances where no alternatives to direct government intervention are available.\nMaximising opportunities for win–win outcomes by expanding the boundaries within which such opportunities are available and encouraging business to go ‘beyond compliance’ within existing legal requirements.\n\n\n\n\nEnforcement Pyramid that builds up uild in regulatory responsiveness. Source: Neil Gunningham and Darren Sinclair\n\n\nThe trick however is finding the right combinations of instruments that work together.\nIn the article ‘Regulatory Pluralism: Designing Policy Mixes for Environmental Protection’, the authors outlined some of the complementary instrument choices that I think can be leveraged for designing AI related regulations.\n\nVoluntarism and Command and Control Regulation: The combination of voluntarism (where individual firms voluntarily seek to improve an objective/target) and traditional command-and-control regulation where there is a clear definition of what is permitted and what is illegal. An example that I can think of is many organizations have been voluntarily reporting on their climate-related disclosures using the Global Reporting Initiative (GRI) sustainability standards in the early 2000s, long before some of the regulations such as the Corporate Sustainability Reporting Directive (CSRD) came into effect where there are clear requirements for organizations on these disclosures.\nAn argument can be made that if voluntarism were introduced alone, the outcomes (in this case, transparent disclosures) would had been sub-optimal and it would had resulted in an increased greenwashing risk. However, the combination of command and control regulations with voluntary standards encourages firms to go beyond compliance while still complying with the performance baseline.\nIn one of my previous posts, I talked about ‘AI Washing’ and how there is a growing movement to open source AI governance. A regulation that combines mandatory requirements such as disclosures of data sources and decision making processes in the AI systems used by organizations along with voluntary initiatives where organizations are encouraged to adopt best practices for AI transparency and fairness. In a way, firms choosing to voluntary adopt these best practices allows this industry to self regulate.\nCommand and Control Regulation and Supply-Side Incentives:\nSupply-side incentives in this context refer to tax concessions or soft loans for achieving specific outcomes. For instance, organizations that meet or exceed the regulatory standards, could be offered grants and tax credits which would encourage them to invest in research initiatives on improving accuracy, transparency and reducing biases of AI systems.\nCommand and Control Regulation and Broad-Based Economic Incentives\nBesides supply-side incentives, command and control regulations could be paired with broader economic incentives in cases when hey are used to target different aspects of a common problem. For example, a regulatory body might impose strict guidelines on AI developers requiring transparency in how algorithms are trained and tested for bias. This might require that organizations to undergo fairness audits and provide detailed reports on the data used and the results of bias evaluations. Failure to meet certain standards would result in penalties.\n\nOn the other hand, some instrument choices might be inherently incompatible. For example, mandating specific AI technologies (command-and-control) for firms in tandem with a tax-based incentive for using a specific technology (economic instruments) might be counterproductive. This is largely because command-and-control regulations seek to impose predetermined outcomes outcomes on industry which means individual firms are largely limited in what they can and cannot do. Economic instruments, on the other hand, seek to maximize the flexibility of firms in their ability to choose the optimal course of action as long as it meets the price signal relative to the level of outcome desired.\nSimilarly, self regulation when combined together with broad-based economic instruments might be an overkill. For example, If an industry code sets rigid data privacy requirements for AI systems across all sectors, it could undermine the effectiveness of the tax penalties where organizations that do not meet the privacy standards. The tax might be less effective in driving firms with higher abatement costs to comply with privacy standards since the self-regulation approach imposes uniform requirements, not considering these cost differences. As a result, the tax may fail to achieve its intended goal of encouraging privacy improvements across the industry.\nTo overcome the drawbacks that may arise from combining instruments, the authors recommend sequencing their introduction, which means certain instruments should be held in reserve and should only be applied if and when other instruments demonstrably fail to meet pre- determined performance benchmarks. It could be as simple as introductng an entirely new instrument category introduced where previous categories have failed or enforcing elements of a pre-existing instrument to supplement the shortcomings of another. Sequencing ensures that there is a progression when it comes to levels of intervention.\nA couple of caveats that I want to mention briefly. Some authors authors claim that smart regulations do not address institutional issues, compliance-type specific responses, performance sensitivity and adaptability of regulatory regimes (Baldwin and Black 2008; Böcher and Toller 2003). Additionally, when discussing how to best regulate AI, it’s important to highlight that it goes far beyond mere technical oversight. Effective regulation must tackle deeply embedded issues such as bias, discrimination, and ethics; something that I did not talk about.\nAre smart regulations the panacea for all the problems? I don’t think so. However, they can be a valuable component of a broader regulatory strategy when it comes to AI governance."
  },
  {
    "objectID": "posts/2024-10-19/index.html",
    "href": "posts/2024-10-19/index.html",
    "title": "Uncertainties in Climate Risk Modelling",
    "section": "",
    "text": "Weather forecasts and climate predictions have been a part of human culture since time immemorial. From elaborate rain dance rituals to simulation models, predicting what the weather and climate is going to be in the near future has always been part of human history. 2024 has been the wild west when it comes to extreme weather and climate events. From record temperatures to hurricanes across the globe, we can’t help but wonder what’s next? However, our predictions about future events is limited by the element of uncertainty. Consider these exhibits:\n\nExhibit A: A while back I was reading this amazing deep dive on the nuances of climate risk modelling. In case you have not read it already, I would highly encourage you to take a look at it. The authors looked at two models - one from UC Irvine and one from First Street - showing areas in Los Angeles County that are vulnerable to flooding and found that outputs from the models were “nearly polar opposite”.\nExhibit B: Earlier this summer, Copernicus, EU’s climate science agency said July was the second hottest month ever recorded. But later, NASA and the US National Oceanic and Atmospheric Administration (NOAA) said August was the world’s hottest month ever recorded. Granted the two both agencies use different methods for measuring temperature, but it still alludes to the fact that there are discrepancies and uncertainties when it comes to measuring weather patterns and long-term climate.\nExhibit C: Figuring out the exact magnitude of extreme events such as Hurricanes Helene and Milton has been an area that forecasters and scientists are still struggling with.\nExhibit D: The concept of “net zero” emissions introduced in Special Report on Global Warming of 1.5 °C (SR15) : “Reaching and sustaining net zero global anthropogenic [human-caused] CO2 emissions and declining net non-CO2 radiative forcing would halt anthropogenic global warming on multi-decadal timescales (high confidence).”\nExhibit E: The estimate ‘bands’ in the global greenhouse gas emissions and warming scenarios:\n\n\n\n\nGlobal greenhouse gas emissions and warming scenarios. Source: Our World In Data\n\n\nAs we see across these exhibits, forecasts and estimates are not absolute and carry an element of uncertainty. It makes me wonder - wouldn’t it be nice if there was a way we could accurately predict when the next extreme weather event is going to happen and map out the exact impacts down to a T? Think of all the collateral damage that can be prevented if there was a magic crystal ball that helped us.\nTurns out all measurements have some degree of uncertainty. As Leonard Smith mentions in the article ‘What might we learn from climate models?’ :\n\nEven in high school physics, we learn that an answer without “error bars” is no answer at all.\n\nWhen we make a measurement, it is based on the assumption that the exact value exists based on how we define what is being measured. We attempt to find this ideal quantity to the best of our ability. However, despite our best efforts, there is always an element of uncertainty introduced in our measurements which may come from a variety of sources such as:\n\nFailure to account for a factor/input variable\nInstrument resolution\nCalibration\nParallax\nInstrument drift\nLag time and hysteresis\nPersonal errors\n\nIn case you are interested in reading more these errors, do check out this chapter on Measurements and Error Analysis by University of North Carolina.\nThis begs the question - how do we report our findings for our best estimate of this true value? From a statistical perspective, the most common way to show the range of values that we believe includes the true value can be represented as\n\\(Measurement = Estimate ± Uncertainty\\)\nFor example, let’s suppose that we want to measure temperature (denoted as a variable \\(Y\\)) and it is an output of different input factors/variables such as humidity, wind speed, pressure etc (denoted as \\(X_1, X_2, X_3...X_p\\)) and there is some relationship between temperature and the inputs humidity, wind speed, pressure. As a result, the relationship can be denoted as:\n\\(Y = f(X) + 𝞮\\)\nHere \\(f\\) is some fixed but unknown function of \\(X_1, X_2, X_3\\) and 𝞮 is a random error term, which is independent of \\(X\\) and has mean zero. As noted in Introduction to Statistical Learning:\n\nThe accuracy of \\(Ŷ\\) as a prediction for \\(Y\\) depends on two quantities, which we will call the reducible error and the irreducible error. In general, reducible \\(\\hat{f}\\) will not be a perfect estimate for \\(\\hat{f}\\) , and this inaccuracy will introduce error some error. This error is reducible because we can potentially improve the irreducible accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to error estimate . However, even if it were possible to form a perfect estimate for \\(f\\) , so that our estimated response took the form \\(Ŷ = f(X)\\), our prediction would still have some error in it! This is because \\(Y\\) is also a function of 𝞮, which, by definition, cannot be predicted using \\(X\\). Therefore, variability associated with 𝞮 also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f\\) is , we cannot reduce the error introduced by 𝞮.\n\nIn other words, the irreducible error will always provide an upper bound on the accuracy of our prediction for temperature and the reality is that the bound is almost always unknown in practice.\nKeep in mind that that predicting weather events and climate is a lot more complicated than a simple linear function as illustrated above. While it is true that over the past few decades we have made strides in quantitatively determining the extent to which climate change played a role in an extreme event, there is still so much we don’t know about them. Turns out the Earth’s climate system is incredibly complex, involving interactions between multiple non-linear systems (including anthropogenic) at play. Then there are measurement errors that are introduced when recording these variables. Work by Patrick Frank at the Stanford Synchrotron Radiation Lightsource (SSRL), has shown that the errors in temperatures recorded by weather stations have been incorrectly handled. Additionally, pretty much all the climate models rely on assumptions and simplifications which can further add uncertainties in the equation.\nIt is also worth noting that climate risk is a function of hazard, exposure and vulnerability.\n\\(Climate Risk = f(Hazard, Exposure, Vulnerability)\\)\nwhere\n\nHazard refers to the possible, future occurrence of natural or human-induced physical events that may have adverse effects on vulnerable and exposed elements;\nExposure refers to the inventory of elements in an area in which hazard events may occur;\nVulnerability refers to the propensity of exposed elements such as human beings, their livelihoods, and assets to suffer adverse effects when impacted by hazard events\n\n\n\n\nClimate Risk as a function of Hazard, Exposure and Vulnerability. Source: Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation\n\n\nSo now not only does one has to factor the probability of these hazards, but also the estimates for exposure and vulnerability which further adds new complexities.\nUltimately, this excerpt from the deep dive done by Roston et al. (2024) on climate risk models nicely sums up the nuances of climate risk modelling:\n\nUnderstanding climate risk is a bit like taking a vision test at the eye doctor. At the largest scale — the big “E” at the top of the chart, or the whole globe — the signs are the clearest. Refined by scientists over decades, climate models have proved very reliable at what they were designed to do, projecting the global effects of rising greenhouse gases.\nBut at progressively smaller scales and over longer time horizons — as the characters on the eye chart shrink — clarity gives way to fuzziness: You’re sure it’s a letter, just not which one. Climate models can be like that. They are simply better at projecting averages than extremes. Outlier events, like 1-in-100-year storms, are still hard to predict.\n\nIt is well known that climate models are not “plug and chug”. At the end of the day, they are simply tools that are used in conjunction with other tools like risk assessments, scenario analysis, computer simulations for decision making purposes. There is work being done to improve these existing processes and reduce the element of uncertainty in climate risk modelling. For instance, many startups like Brightband are leveraging machine learning algorithms to predict the behavior of extreme weather events. Maybe we are not that far away from creating a holy grail of climate risk modelling where we can map out the impacts to a T. But until then, it looks like uncertainties are here to stay with our predictions.\nTo wrap things up, I would like to leave this quote by Yogi Berra that I read in An Introduction to Statistical Learning\n\nIt’s tough to make predictions, especially about the future.\n\n\n\n\nMetereologist. Souce XKCD Comic #1985"
  },
  {
    "objectID": "posts/2024-11-09/index.html",
    "href": "posts/2024-11-09/index.html",
    "title": "AI and Jobs",
    "section": "",
    "text": "AI taking over human jobs is an age old topic. I am old enough to remember watching Push Button Kitty episode of Tom and Jerry as a kid where Tom gets replaced by a robotic cat.\n\nFast forward to 2024 and we have, dockworkers concerned about being replaced by autonomous machines, China completing the world’s first fully unmanned paving construction along a 157.79 km stretch of the Beijing-Hong Kong-Macao Expressway and Tyler Perry halting his $800 million studio expansion project after seeing the capabilities of OpenAI’s Sora.\nThis is a very nuanced topic in itself which partly explains why almost every discussion on this topic can be boiled down to - ‘We Don’t Know’. I have always been curious to understand what is really driving this sentiment which is what the focus of this post is going to be.\nI do want to preface by saying most of this stuff may very well be purely speculative and may read like works of science fiction. When it comes to automation and jobs, there are so many what ifs and assumptions that are driving this ‘We Don’t Know’ sentiment. Achieving Artificial General Intelligence (AGI) in our lifetimes is a big if. No one knows for sure if its going to happen or when its going to happen.\nBefore going into the nitty-gritties, it might be worth taking a step back and understanding what AI in its current state can and can’t do. Something that helped me contextualize this was the concept of ‘Landscape of Human Competence’ that I came across in Max Tegmark’s Life 3.0. Hans Moravec illustrates the potential of AI using the following metaphor:\n\nComputers are universal machines, their potential extends uniformly over a boundless expanse of tasks. Human potentials, on the other hand, are strong in areas long important for survival, but weak in things far removed. Imagine a “landscape of human competence,” having lowlands with labels like “arithmetic” and “rote memorization,” foothills like “theorem proving” and “chess playing,” and high mountain peaks labeled “locomotion,” “hand-eye coordination” and “social interaction.” Advancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose that we build Arks as that day nears, and adopt a seafaring life!\n\n\n\n\nHans Moravec’s illustration of the rising tide of the AI capacity. Source: Life 3.0 by Max Tegmark\n\n\nPut simply when the ‘sea level rises’ up to the peak, in theory we will achieve Artificial General Intelligence. We can see that the tide for capacity has been rising over the past few decades. Computers have been able to do basic arithmetic since mid 20th Century. In 1997, IBM’s Deep Blue chess computer, dethroned chess champion Garry Kasparov. Soon enough, Google’s DeepMind AlphaGo was able to beat Lee Sedol in a five-match game of Go. Today we are seeing breakthroughs in speech recognition, language translation and computer vision problems. There was a time, not too long ago, when the idea of computers doing these seemingly complex tasks seemed impossible. Nick Bostrom explains this in Superintelligence:\n\nThere is an important sense, however, in which chess-playing AI turned out to be a lesser triumph than many imagined it would be. It was once supposed, perhaps not unreasonably, that in order for a computer to play chess at grandmaster level, it would have to be endowed with a high degree of general intelligence. One might have thought, for example, that great chess playing requires being able to learn abstract concepts, think cleverly about strategy, compose flexible plans, make a wide range of ingenious logical deductions, and maybe even model one’s opponent’s thinking. Not so. It turned out to be possible to build a perfectly fine chess engine around a special-purpose algorithm. When implemented on the fast processors that became available towards the end of the twentieth century, it produces very strong play. But an AI built like that is narrow. It plays chess; it can do no other.\nIn other domains, solutions have turned out to be more complicated than initially expected, and progress slower. The computer scientist Donald Knuth was struck that “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking’—that, somehow, is much harder!”Analyzing visual scenes, recognizing objects, or controlling a robot’s behavior as it interacts with a natural environment has proved challenging. Nevertheless, a fair amount of progress has been made and continues to be made, aided by steady improvements in hardware.\n\nThis phenomenon of reasoning requiring less computational resources and other sensorimotor and perception skills requiring enormous computational resources is known as Moravec’s Paradox. This is where the topic of automation and jobs starts getting nuanced. For starters, it is difficult to identify which tasks can AI do. We take it for granted to be able to analyse visual scenes, recognise objects that are difficult (note difficult, not impossible) for an AI to do. On the flip side, we assume that playing chess and Go would be impossible for an AI to do but as we have already seen, AI has already outsmarted us in these domains. That’s why it is hard to accurately predict which areas AI is going to outsmart us in the future.\nThen there is also the AI effect, where the goalposts of real intelligence keeps on moving. Depending on who you ask, people may have different interpretations of what AGI is. Going back to the landscape of tasks, some might argue the sea level of AI capacity is already close to the peak. Critics argue that it is not a given that the sea level will keep on rising and we may never reach a point when computers can do some of the more complicated tasks in the landscape of human competence.\n\nNow the obvious question will be - what do we do about it? There are a couple of high-level observations we can make in the context of AI and automation\n\nThere are areas that AI is very good at and there are areas that AI not very good at (as of today). Given the ambiguities in defining ‘real intelligence’, it’s hard to pinpoint exactly what those areas are. Broadly speaking, AI in its current state excels at a narrow subset of highly repetitive or structured actions in a predictable setting. Max Tegmark jokingly remarks in his TED Talk, that the obvious takeaway is to avoid careers at the waterfront of landscape of tasks. In words of Chamath Palihpitiya, ‘’we shouldn’t set up the future generations to compete with computers [in these areas]’’.\nThere have been some forecasts for when various jobs will get taken over by machines such as the ones by Oxford University, and McKinsey. These forecasts provide a sense of which areas are going to be the most impacted by automation (transportation, logistics, office and administrative and production occupations) are at risk and would require to focus our efforts from a policy-making perspective.\nOn the flip side, skills like creative problem-solving, empathy, negotiation, social intelligence are going to become valuable.\nSo far we have focused our discussion on areas that are going to be automated and the difficulties in classifying them in the first place. But there is another, equally important piece in this equation. And it involves ethics.\nOne could make a case about how automation could be morally justifiable in the case of some of the high risk occupations. Think about how many lives and injuries could be avoided if we have robots doing the grunt work in some of the high risk occupations such as in construction trades?\n\n\n\nCivilian occupations with high fatal work injury rates (in the United States). Source: Bureau of Labor Statistics\n\n\nIn 2022, almost 44,000 people died in motor vehicle crashes in the United States. That is more than 110 people killed in crashes every day. Now imagine how many lives could be saved if we had self-driving cars. Yes, that would eliminate driving professions such as Uber/Lyft drivers, truck drivers. But would it be justifiable?\nWhat about car enthusiasts; people who like going on long drives in the country side? Should they give up driving too?\nWhat about professions that are not as risky, like animators? Maybe Tyler Perry is right and eventually there will be no need for animators as OpenAI’s Sora will be able to whip up something that far exceeds human capabilities. Or maybe not?\nMaybe there is something about human-made creations that resonates with us. I was recently reading The Creative Act by Rick Rubin and he sums it up perfectly:\n\nFlaws are human, and the attraction of art is the humanity held in it. If we were machinelike, the art wouldn’t resonate. It would be soulless.\n\nThere are times when I still enjoy the ‘commercialized’ pizza you get at Domino’s. But there is something to be said about the hand-stretched Neapolitan pizza that they serve at the my local pizza joint that is hard to beat. Maybe AI will never be able to recreate those imperfections that resonate with us.\nAssuming we reach this point in a distant future where all jobs are automated and there is no need for humans to work. The question now remains - how do we sustain that society? Beyond providing a means to living, jobs provide a purpose and a sense of meaning to many people. A Universal Basic Income (which in itself is a very controversial topic) may take care of an individual’s basic needs such as housing, but how do we replace the sense of belonging and community that is present in jobs?\nWhat if we also consider how ethics may apply to AI itself? What if AI develops a conscience and does not want to be subjected to these grunt work? What do we do then? You might be reading this and wondering, ‘’Aryamik, you have seen too many episodes of Westworld. You need to take a chill-pill.’’\nI have to admit this one is something that is hard to wrap my head around. Partly because I have seen some iterations of this unfolding in pop-culture (you guessed it, Westworld). The story is basically the same\n\nHumans deploy robots for their selfish needs.\nRobots became self-aware\nRobots rebel\nSociety turns into a dystopian nightmare\n\nIt is worth mentioning that in Life 3.0, Max Tegmark warns against the temptation to anthropomorphize AIs and assume that they have typical human-like feelings. Like I said earlier, its not even a given that AI will ever reach that stage in our lifetimes where we will have to think about giving rights to AI. In fact, AI researcher Jeff Hawkins argues in his book On Intelligence that the first machines with AGI will lack emotions by default and as a result it might be possible to design a superintelligence whose enslavement is justified. In fact concepts like consciousness and experiences may not even apply to AGI.\nHow long will it take until machines can reach a point of consciousness? We clearly don’t know, and need to be open to the possibility that the answer may be “never.” However, we also need to consider the possibility that it will happen, perhaps even in our lifetime.\nIn short, the technological complexities as well as some of the aforementioned ethical considerations makes automation and jobs such a nuanced topic. It’s worth noting that there are more variables in this conversation that I didn’t cover such as economic considerations, which adds a whole set of complexities.\nI will wrap things up by sharing this metaphor from Scottish-American economist Gregory Clark in his book Farewell to Alms:\nImagine two horses looking at an early automobile in the year 1900 and pondering their future.\n\n“I’m worried about technological unemployment.”\n“Neigh, neigh, don’t be a Luddite: our ancestors said the same thing when steam engines took our industry jobs and trains took our jobs pulling stage coaches. But we have more jobs than ever today, and they’re better too: I’d much rather pull a light carriage through town than spend all day walking in circles to power a stupid mine-shaft pump.”\n“But what if this internal combustion engine thing really takes off?”\n“I’m sure there’ll be new new jobs for horses that we haven’t yet imagined. That’s what’s always happened before, like with the invention of the wheel and the plow.”\n\n\nAre we destined to follow the same fate as the equines? Or will we get the digital utopia we have always yearned for? One thing is for certain: assuming we continue to make developments in AI over the next few decades, its impact on society will keep growing and it will give us fascinating opportunities and challenges."
  },
  {
    "objectID": "posts/2024-11-24/index.html",
    "href": "posts/2024-11-24/index.html",
    "title": "AI and Jobs",
    "section": "",
    "text": "AI taking over human jobs is an age old topic. I am old enough to remember watching Push Button Kitty episode of Tom and Jerry as a kid where Tom gets replaced by a robotic cat.\n\nFast forward to 2024 and we have, dockworkers concerned about being replaced by autonomous machines, China completing the world’s first fully unmanned paving construction along a 157.79 km stretch of the Beijing-Hong Kong-Macao Expressway and Tyler Perry halting his $800 million studio expansion project after seeing the capabilities of OpenAI’s Sora.\nThis is a very nuanced topic in itself which partly explains why almost every discussion on this topic can be boiled down to - ‘We Don’t Know’. I have always been curious to understand what is really driving this sentiment which is what the focus of this post is going to be.\nI do want to preface by saying most of this stuff may very well be purely speculative and may read like works of science fiction. When it comes to automation and jobs, there are so many what ifs and assumptions that are driving this ‘We Don’t Know’ sentiment. Achieving Artificial General Intelligence (AGI) in our lifetimes is a big if. No one knows for sure if its going to happen or when its going to happen.\nBefore going into the nitty-gritties, it might be worth taking a step back and understanding what AI in its current state can and can’t do. Something that helped me contextualize this was the concept of ‘Landscape of Human Competence’ that I came across in Max Tegmark’s Life 3.0. Hans Moravec illustrates the potential of AI using the following metaphor:\n\nComputers are universal machines, their potential extends uniformly over a boundless expanse of tasks. Human potentials, on the other hand, are strong in areas long important for survival, but weak in things far removed. Imagine a “landscape of human competence,” having lowlands with labels like “arithmetic” and “rote memorization,” foothills like “theorem proving” and “chess playing,” and high mountain peaks labeled “locomotion,” “hand-eye coordination” and “social interaction.” Advancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose that we build Arks as that day nears, and adopt a seafaring life!\n\n\n\n\nHans Moravec’s illustration of the rising tide of the AI capacity. Source: Life 3.0 by Max Tegmark\n\n\nPut simply when the ‘sea level rises’ up to the peak, in theory we will achieve Artificial General Intelligence. We can see that the tide for capacity has been rising over the past few decades. Computers have been able to do basic arithmetic since mid 20th Century. In 1997, IBM’s Deep Blue chess computer, dethroned chess champion Garry Kasparov. Soon enough, Google’s DeepMind AlphaGo was able to beat Lee Sedol in a five-match game of Go. Today we are seeing breakthroughs in speech recognition, language translation and computer vision problems. There was a time, not too long ago, when the idea of computers doing these seemingly complex tasks seemed impossible. Nick Bostrom explains this in Superintelligence:\n\nThere is an important sense, however, in which chess-playing AI turned out to be a lesser triumph than many imagined it would be. It was once supposed, perhaps not unreasonably, that in order for a computer to play chess at grandmaster level, it would have to be endowed with a high degree of general intelligence. One might have thought, for example, that great chess playing requires being able to learn abstract concepts, think cleverly about strategy, compose flexible plans, make a wide range of ingenious logical deductions, and maybe even model one’s opponent’s thinking. Not so. It turned out to be possible to build a perfectly fine chess engine around a special-purpose algorithm. When implemented on the fast processors that became available towards the end of the twentieth century, it produces very strong play. But an AI built like that is narrow. It plays chess; it can do no other.\nIn other domains, solutions have turned out to be more complicated than initially expected, and progress slower. The computer scientist Donald Knuth was struck that “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking’—that, somehow, is much harder!”Analyzing visual scenes, recognizing objects, or controlling a robot’s behavior as it interacts with a natural environment has proved challenging. Nevertheless, a fair amount of progress has been made and continues to be made, aided by steady improvements in hardware.\n\nThis phenomenon of reasoning requiring less computational resources and other sensorimotor and perception skills requiring enormous computational resources is known as Moravec’s Paradox. This is where the topic of automation and jobs starts getting nuanced. For starters, it is difficult to identify which tasks can AI do. We take it for granted to be able to analyse visual scenes, recognise objects that are difficult (note difficult, not impossible) for an AI to do. On the flip side, we assume that playing chess and Go would be impossible for an AI to do but as we have already seen, AI has already outsmarted us in these domains. That’s why it is hard to accurately predict which areas AI is going to outsmart us in the future.\nThen there is also the AI effect, where the goalposts of real intelligence keeps on moving. Depending on who you ask, people may have different interpretations of what AGI is. Going back to the landscape of tasks, some might argue the sea level of AI capacity is already close to the peak. Critics argue that it is not a given that the sea level will keep on rising and we may never reach a point when computers can do some of the more complicated tasks in the landscape of human competence.\n\nNow the obvious question will be - what do we do about it? There are a couple of high-level observations we can make in the context of AI and automation\n\nThere are areas that AI is very good at and there are areas that AI not very good at (as of today). Given the ambiguities in defining ‘real intelligence’, it’s hard to pinpoint exactly what those areas are. Broadly speaking, AI in its current state excels at a narrow subset of highly repetitive or structured actions in a predictable setting. Max Tegmark jokingly remarks in his TED Talk, that the obvious takeaway is to avoid careers at the waterfront of landscape of tasks. In words of Chamath Palihpitiya, ‘’we shouldn’t set up the future generations to compete with computers [in these areas]’’.\nThere have been some forecasts for when various jobs will get taken over by machines such as the ones by Oxford University, and McKinsey. These forecasts provide a sense of which areas are going to be the most impacted by automation (transportation, logistics, office and administrative and production occupations) are at risk and would require to focus our efforts from a policy-making perspective.\nOn the flip side, skills like creative problem-solving, empathy, negotiation, social intelligence are going to become valuable.\nSo far we have focused our discussion on areas that are going to be automated and the difficulties in classifying them in the first place. But there is another, equally important piece in this equation. And it involves ethics.\nOne could make a case about how automation could be morally justifiable in the case of some of the high risk occupations. Think about how many lives and injuries could be avoided if we have robots doing the grunt work in some of the high risk occupations such as in construction trades?\n\n\n\nCivilian occupations with high fatal work injury rates (in the United States). Source: Bureau of Labor Statistics\n\n\nIn 2022, almost 44,000 people died in motor vehicle crashes in the United States. That is more than 110 people killed in crashes every day. Now imagine how many lives could be saved if we had self-driving cars. Yes, that would eliminate driving professions such as Uber/Lyft drivers, truck drivers. But would it be justifiable?\nWhat about car enthusiasts; people who like going on long drives in the country side? Should they give up driving too?\nWhat about professions that are not as risky, like animators? Maybe Tyler Perry is right and eventually there will be no need for animators as OpenAI’s Sora will be able to whip up something that far exceeds human capabilities. Or maybe not?\nMaybe there is something about human-made creations that resonates with us. I was recently reading The Creative Act by Rick Rubin and he sums it up perfectly:\n\nFlaws are human, and the attraction of art is the humanity held in it. If we were machinelike, the art wouldn’t resonate. It would be soulless.\n\nThere are times when I still enjoy the ‘commercialized’ pizza you get at Domino’s. But there is something to be said about the hand-stretched Neapolitan pizza that they serve at the my local pizza joint that is hard to beat. Maybe AI will never be able to recreate those imperfections that resonate with us.\nAssuming we reach this point in a distant future where all jobs are automated and there is no need for humans to work. The question now remains - how do we sustain that society? Beyond providing a means to living, jobs provide a purpose and a sense of meaning to many people. A Universal Basic Income (which in itself is a very controversial topic) may take care of an individual’s basic needs such as housing, but how do we replace the sense of belonging and community that is present in jobs?\nWhat if we also consider how ethics may apply to AI itself? What if AI develops a conscience and does not want to be subjected to these grunt work? What do we do then? You might be reading this and wondering, ‘’Aryamik, you have seen too many episodes of Westworld. You need to take a chill-pill.’’\nI have to admit this one is something that is hard to wrap my head around. Partly because I have seen some iterations of this unfolding in pop-culture (you guessed it, Westworld). The story is basically the same\n\nHumans deploy robots for their selfish needs.\nRobots became self-aware\nRobots rebel\nSociety turns into a dystopian nightmare\n\nIt is worth mentioning that in Life 3.0, Max Tegmark warns against the temptation to anthropomorphize AIs and assume that they have typical human-like feelings. Like I said earlier, its not even a given that AI will ever reach that stage in our lifetimes where we will have to think about giving rights to AI. In fact, AI researcher Jeff Hawkins argues in his book On Intelligence that the first machines with AGI will lack emotions by default and as a result it might be possible to design a superintelligence whose enslavement is justified. In fact concepts like consciousness and experiences may not even apply to AGI.\nHow long will it take until machines can reach a point of consciousness? We clearly don’t know, and need to be open to the possibility that the answer may be “never.” However, we also need to consider the possibility that it will happen, perhaps even in our lifetime.\nIn short, the technological complexities as well as some of the aforementioned ethical considerations makes automation and jobs such a nuanced topic. It’s worth noting that there are more variables in this conversation that I didn’t cover such as economic considerations, which adds a whole set of complexities.\nI will wrap things up by sharing this metaphor from Scottish-American economist Gregory Clark in his book Farewell to Alms:\nImagine two horses looking at an early automobile in the year 1900 and pondering their future.\n\n“I’m worried about technological unemployment.”\n“Neigh, neigh, don’t be a Luddite: our ancestors said the same thing when steam engines took our industry jobs and trains took our jobs pulling stage coaches. But we have more jobs than ever today, and they’re better too: I’d much rather pull a light carriage through town than spend all day walking in circles to power a stupid mine-shaft pump.”\n“But what if this internal combustion engine thing really takes off?”\n“I’m sure there’ll be new new jobs for horses that we haven’t yet imagined. That’s what’s always happened before, like with the invention of the wheel and the plow.”\n\n\nAre we destined to follow the same fate as the equines? Or will we get the digital utopia we have always yearned for? One thing is for certain: assuming we continue to make developments in AI over the next few decades, its impact on society will keep growing and it will give us fascinating opportunities and challenges."
  }
]