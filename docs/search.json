[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Sympatheia",
    "section": "",
    "text": "Can Data Explain my Musical Tastes?\n\n\n\nData\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nSep 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCan Data Explain my Musical Tastes?\n\n\n\nData\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nSep 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIs AI an ‘existential risk’?\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we regulate AI\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nAug 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI Washing - Is it time to put a brake on the AI Hype Train?\n\n\n\nAI\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Sustainability - How to make them coexist?\n\n\n\nAI\n\n\nSustainability\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGoodhart’s Law - When Chasing Measures Goes Wrong\n\n\n\nEconomics\n\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBare Beginnings\n\n\n\n\n\n\nAryamik Sharma\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Linkedin\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website!\nAs a sustainability professional, I enjoy connecting the dots: be it ideas from different disciplines, people from different teams, or applications from different industries. I have strong technical skills and an academic background in ESG, data analytics, and statistics.\nMy passion lies in exploring the connection between financial sector players and the link between sustainability and financial performance of enterprises. I excel when working in collaborative groups to solve real-world business problems and am eager to adapt and learn in different environments.\nWhen I am not in the office I love playing my bass guitar, pursuing my fitness goals or baking new treats! Please feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "photography/index.html",
    "href": "photography/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-languages-of-middle-earth",
    "href": "projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "posts/2023-12-18- Actual Post/index.html",
    "href": "posts/2023-12-18- Actual Post/index.html",
    "title": "Aryamik Sharma",
    "section": "",
    "text": "Hi There, this is for the actual post.\n\nimport numpy as np\n\n\n#importing libraries\nimport altair as alt\nfrom bokeh.sampledata.penguins import data as penguins\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#plotting\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(\n  data=penguins, \n  title=\"Palmer Penguins Dataset\",\n  ).mark_circle(size=60).encode(\n  alt.X('bill_length_mm', scale=alt.Scale(domain=[30,60])),\n  alt.Y('bill_depth_mm', scale=alt.Scale(domain=[12,22])),\n  color='species',\n  ).add_selection(\n    brush\n)\n\nbars = alt.Chart(penguins).mark_bar().encode(\n    y='island',\n    color='island',\n    x='count(island)'\n).transform_filter(\n    brush\n)\n\npoints & bars\n\n/home/aryamik/anaconda3/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning:\n\n'selection' is deprecated.\n   Use 'selection_point()' or 'selection_interval()' instead; these functions also include more helpful docstrings.\n\n/home/aryamik/anaconda3/lib/python3.10/site-packages/altair/utils/deprecation.py:65: AltairDeprecationWarning:\n\n'add_selection' is deprecated. Use 'add_params' instead."
  },
  {
    "objectID": "posts/2023-12-18-Second Pst/index.html",
    "href": "posts/2023-12-18-Second Pst/index.html",
    "title": "First Post",
    "section": "",
    "text": "#importing libraries\nimport altair as alt\nfrom bokeh.sampledata.penguins import data as penguins\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#plotting\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(\n  data=penguins, \n  title=\"Palmer Penguins Dataset\",\n  ).mark_circle(size=60).encode(\n  alt.X('bill_length_mm', scale=alt.Scale(domain=[30,60])),\n  alt.Y('bill_depth_mm', scale=alt.Scale(domain=[12,22])),\n  color='species',\n  ).add_selection(\n    brush\n)\n\nbars = alt.Chart(penguins).mark_bar().encode(\n    y='island',\n    color='island',\n    x='count(island)'\n).transform_filter(\n    brush\n)\n\npoints & bars\nHi there\nHave you guys ever seen that meme?\nThat’s me\nSo you are all wondering how does a random guy hop on this blogging platform.\nEverything below this is a test to see if the website is actually working."
  },
  {
    "objectID": "posts/2023-12-18-first-post/index.html",
    "href": "posts/2023-12-18-first-post/index.html",
    "title": "Bare Beginnings",
    "section": "",
    "text": "Hey there!\nWelcome to Sympatheia, my very own personal project that I am very excited to share with everyone!\nYou might be wondering what is this project all about or what does the word Sympatheia mean in the first place?\nA couple of years ago when I was reading Meditations by Marcus Aurelius (which is easily one of my favorite books of all time and would highly recommend that you read it atleast once!), I came across this amazing excerpt:\n\nMeditate often on the interconnectedness and mutual interdependence of all things in the universe. For in a sense, all things are mutually woven together and therefore an affinity for each other - for one thing follows after another according to their tension of movement, their sympathetic stirrings, and the unity of all substance.\n\nUpon further research, I found that the Stoics had a word for it - sympatheia - a connectedness with the cosmos.\nThis concept of interconnectedness is something that deeply resonates with me as I really enjoy connecting the dots from various disciplines in my own life.\nYou could think of Sympatheia as my own personal journal where I aim to explore the interconnectedness of some of the areas that I am really passionate about such as:\n\nESG and Sustainability\nData Science\nMachine Learning and Artificial Intelligence\nPhilosophy and Ethics\nFinance and Economics\n\nAt the end of the day it is about my own journey as I try to decipher these seemingly complexy topics and they can possibly interact with each other.\nSo now that you know a little bit about what this project is all about, I hope you are as intrigued and excited as I am. Your comments, feedback, and suggestions are invaluable to me, so feel free to share them. Stay tuned for more insights and updates!"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html",
    "href": "posts/2024-06-28-JRE/index.html",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "",
    "text": "I was listening to Episode #2156 of the Joe Rogan Experience featuring Jeremie & Edouard Harris from Gladstone AI, an organization dedicated to promoting the responsible development and adoption of AI. The whole episode had interesting tidbits that I really liked but for this post, I wanted to focus on just one segment - the economic principle of ‘Goodhart’s law’. They discuss it around 56:30:\nThis led me into a rabbit hole of Goodhart’s law and what it is all about. It can be traced back to 1975 when the British economist Charles Goodhart expressed this idea in his article “Problems of Monetary Management: The U.K. Experience”. In the article Goodhart explains:\nSimply put\nGoodhart’s law was originally developed in the context of conducting monetary policy on the basis of targets. However, its application can be observed in many other areas. Here are some other applications of Goodhart’s law that I can think of:"
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "Disclaimer",
    "section": "",
    "text": "The information provided on this website reflects my personal thoughts, opinions, and experiences. It is intended for informational purposes only and should not be taken as professional advice. All information on the site is provided in good faith, however, I make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability, availability, or completeness of any information on the site.\nAll views expressed on this site are my own and do not represent the opinions of any entity with which I have been or am currently affiliated."
  },
  {
    "objectID": "posts/2024-07-01/index.html",
    "href": "posts/2024-07-01/index.html",
    "title": "Bare Beginnings",
    "section": "",
    "text": "Hey there!\nWelcome to Sympatheia, my very own personal project that I am very excited to share with everyone!\nYou might be wondering what is this project all about or what does the word Sympatheia mean in the first place?\nA couple of years ago when I was reading Meditations by Marcus Aurelius (which is easily one of my favorite books of all time and would highly recommend that you read it atleast once!), I came across this amazing excerpt:\n\nMeditate often on the interconnectedness and mutual interdependence of all things in the universe. For in a sense, all things are mutually woven together and therefore an affinity for each other - for one thing follows after another according to their tension of movement, their sympathetic stirrings, and the unity of all substance.\n\nUpon further research, I found that the Stoics had a word for it - sympatheia - a connectedness with the cosmos.\nThis concept of interconnectedness is something that deeply resonates with me as I really enjoy connecting the dots from various disciplines in my own life.\nYou could think of Sympatheia as my own personal journal where I aim to explore the interconnectedness of some of the areas that I am really passionate about such as:\n\nESG and Sustainability\nData Science\nMachine Learning and Artificial Intelligence\nPhilosophy and Ethics\nFinance and Economics\n\nAt the end of the day it is about my own journey as I try to decipher these seemingly complexy topics and how they can possibly interact with each other.\nSo now that you know a little bit about what this project is all about, I hope you are as intrigued and excited as I am. Your comments, feedback, and suggestions are invaluable to me, so feel free to share them. Stay tuned for more insights and updates!"
  },
  {
    "objectID": "Disclaimer.html",
    "href": "Disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "The information provided on this website reflects my personal thoughts, opinions, and experiences. It is intended for informational purposes only and should not be taken as professional advice. All information on the site is provided in good faith, however, I make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability, availability, or completeness of any information on the site.\nAll views expressed on this site are my own and do not represent the opinions of any entity with which I have been or am currently affiliated."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#my-fitness-journey",
    "href": "posts/2024-06-28-JRE/index.html#my-fitness-journey",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "My Fitness Journey",
    "text": "My Fitness Journey\nAfter hearing about Goodhart’s law, It got me thinking about my own fitness journey. When I was just starting out, I used to get so fixated on the number on weighing scale because in my head I rationalized it as:\nLower weight on the weighing scale = Being closer to my fitness goals.\nI became so obsessed with the number that I saw on the weighing scale everyday that I started ‘gaming’ the system. I started pulling off some shenanigans like being over zealous with my cardio, being extremely restrictive with my calories so that the next day the scale would read the number that I wanted to see. It got to a point where I lost track of my broader fitness goals and why I was doing it in the first place - to stay fit and be more healthy.\nIt wasn’t until many years later that I realized that I was doing it all wrong. It occurred me to that weight is one of the many metrics that can be used to gauge your progress. It can serve as a proxy for your fitness journey but it is not the end all be all metric that is worth obsessing over. Not only that, but relying solely on a weight loss scale for fitness progress can be misleading. Turns out many variables affect your body weight, such as:\n\nSleep\nCarbohydrate intake\nSodium intake\nFiber\nMenstrual cycle\nFood volume\nConstipation\nStress\nHydration\n\nTherefore even if you are giving 100%, the numbers on weighing scale can fluctuate easily.\nSo what does this tell? Being obsessed with chasing a particular metric (in this case the number on weighing scale) ends up telling nothing about what I wanted to measure in the first place."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#ai-goals",
    "href": "posts/2024-06-28-JRE/index.html#ai-goals",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "AI Goals",
    "text": "AI Goals\nIn Life 3.0 (which is easily one of my favorite books of all time), Max Tegmark mentions that in our quest to building Artificial General Intelligence (AGI) as we get more intelligent and powerful machines, it will become paramount to ensure that their goals are aligned with ours. He argues that figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard and continues to be an unsolved problem. For example, if I ask a self-driving car to take me to the airport as fast as possible and it takes my word for it, the next thing I know I am being chased by helicopters and I find myself covered in vomit. I could get mad at the car all day long and tell it ‘That’s not what I wanted!’, it can argue ‘That’s what you asked for.’"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#sustainability-targets-and-initiatives",
    "href": "posts/2024-06-28-JRE/index.html#sustainability-targets-and-initiatives",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Sustainability Targets and Initiatives",
    "text": "Sustainability Targets and Initiatives\nLet’s take a look at another example of where Goodhart’s law maybe applicable - sustainability targets and initiatives.\nFollowing the Paris Agreement, the Intergovernmental Panel on Climate Change published its Special Report on Global Warming of 1.5 °C (SR15) in which stated that\n\n“Reaching and sustaining net zero global anthropogenic [human-caused] CO2 emissions and declining net non-CO2 radiative forcing would halt anthropogenic global warming on multi-decadal timescales.\n\nEver since then, there has been significant growth in the number of actors pledging net zero emissions. Many standards such as the Science Based Targets initiative (SBTi) and International Sustainability Standards have emerged that interpret the net zero concept and aim to measure progress towards net zero targets. The rise of these standards have led to the (re)emergence of various policies and mechanisms such as carbon offsets.\nThe idea behind a carbon offset is that when an entity releases greenhouse gases, they can pay someone else to remove an equivalent amount of climate pollution from the atmosphere. For example, if an automotive manufacturer in a developed country that wants to claim it is reducing its emissions, the manufacturer can pay for a patch of rainforest to be protected in the Amazon that can absorb an equal amount of emissions. This in theory – cancels out - or offsets the impact of the automotive manufacturer emissions.\nOne can see the appeal for carbon offsets. For many industries, the cost of addressing their own emissions can be a huge undertaking. By directly funding offset projects, the cost of addressing climate change becomes more manageable. It sounds pretty straightforward, right?\nIt is all fun and games until you take a look at the reality.\nHow it started:\n\n\n\nSource: Marketplace.org\n\n\nHow it’s going:\n\n\n\nSource: Bloomberg.org\n\n\nCarbon offsets have been there since 1970s. So what went wrong along the way?\nKeeping aside the limitations of carbon offsets such as overstated baselines, double counting of emissions, additionality; one can see how Goodhart’s law might be at play here.\nConsider a simple example. A dairy farm wants to setup its operations in California. This means the dairy farm will have to clear 1 hectare of land to be up and running. Theoretically, the dairy farm could offset its impact by planting 1 hectare of vegetation in Amazon. Soon enough, this will prompt other dairy farms to do the same thing and before you know it hundreds of acres of lands have been cleared in California. But you might say : ‘Aren’t they making up for their impacts by offsetting their emissions in Amazon?’\nPartially yes. But 1 hectare of land in California is not the same as 1 hectare of land in Amazon. This has been one of the criticisms of carbon offsets where offset projects tend to encourage ‘business as usual’ practices without considering the ecological complexities of the different regions.\nThe organizations can claim to be carbon neutral (which on paper they might be). But in the end, are we really ‘solving’ for the underlying problem?"
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#biodiversity-conservation",
    "href": "posts/2024-06-28-JRE/index.html#biodiversity-conservation",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Biodiversity Conservation",
    "text": "Biodiversity Conservation\nSimilarly, Goodhart’s law finds it way in a related area - protecting biodiversity.\nRapid declines in populations of various vulnerable species have led to their inclusion in direct measures of biodiversity such as International Union for Conservation of Nature (IUCN), Red List Index (RLI). The Index is based on the IUCN Red List of Threatened Species™, which is widely considered to be the leading assessment of the extinction risk of species. The Red List involves the application of quantitative criteria based on population size, distribution area, and rate of decline, to assign species to different categories of relative extinction risk.\nAs a result, a substantial research effort has been devoted to examining the causes of decline in these vulnerable species. This has led to increases in conservation efforts which have had some success in slowing or even reversing the observed declines. At first glance, this can viewed as a success story. However, it has also potentially undermined the use of these species as a sustainability indicator. As policy and management interventions have focused specifically on vulnerable species, with the aim of improving this indicator, abundance of these species is arguably now less representative of the general state of the natural environment than it was hitherto. Consequently, any increase in the indicator is more likely to be a measure of specific response actions than of any general improvement in the state of the environment."
  },
  {
    "objectID": "posts/2024-06-28-JRE/index.html#way-forward",
    "href": "posts/2024-06-28-JRE/index.html#way-forward",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Way Forward",
    "text": "Way Forward\nThese examples illustrate the central theme of Goodhart’s law, that is: once a measure is declared as important and policy aims to reduce it, the underlying correlation will be reduced.\nThis post is not meant to be a criticism of carbon offsets, sustainability standards or measures as a whole. As someone who is part of the ‘what gets measured, gets managed’ crowd, I still believe that measures still serve an important purpose.\nBut given what we know about Goodhart’s law, two key questions emerge:\n\nWhy does it happen?\nWhat can we do about it?\n\nIn the episode, Jeremie explains this happens because you end up baking in some misalignment between what you want and what the system wants. The more powerful that system becomes the the more it exploits that gap. People will tend to affect any given indicator/measure in whichever ways can be most readily achieved. As a result, indicators become decoupled from underlying process that they are supposed to indicate, and indicator values will become artificially inflated without addressing the underlying problem.\nThat takes care of the ‘why it happens’ part. Now how do we actually address it?\nBefore coming up with ways to quantify something, we should be really clear with what are we trying to solve for? Do I want to track my daily weight or do I want to be the healthiest version of myself? Do I want to offset my emissions or do I want to make sure the global ecosystems are protected?\nOnce we are clear on the problem we are trying to solve, we should then think of all the possible ways in which the given indicator/measure could be exploited/gamified. How could our measure of choice become decoupled from the underlying process?\nSometimes we won’t have clear answers to to these questions, especially when we are trying to solve a novel problem where there has been no precedence. However, considering some of the confounding variables at the onset of the process could help us ensure that our measures are aligned with what we are trying to solve for.\nFinally, systems should be put in place to prevent manipulation of the indicators and the assessments on which they are based, to ensure that the information they provide is objective and reliable. Using multiple indicator sets, including measures of pressure as well as state variables, could help reduce scope for indicator manipulation.\nIn conclusion, measures serve an important function in guiding decision-making and assessing progress toward goals. However, there is a need for caution in interpreting the information provided by any measure that are used; at best, they can only provide a partial indication of the status and trends of the process we are trying to assess, and this needs to be appreciated by the decision-makers who employ them."
  },
  {
    "objectID": "posts/2024-07-07/index.html",
    "href": "posts/2024-07-07/index.html",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "",
    "text": "I was listening to Episode #2156 of the Joe Rogan Experience featuring Jeremie & Edouard Harris from Gladstone AI, an organization dedicated to promoting the responsible development and adoption of AI. The whole episode had interesting tidbits that I really liked but for this post, I wanted to focus on just one segment - the economic principle of ‘Goodhart’s law’. They discuss it around 56:30:\nThis led me into a rabbit hole of Goodhart’s law and figuring out what it is all about. It can be traced back to 1975 when the British economist Charles Goodhart expressed this idea in his article “Problems of Monetary Management: The U.K. Experience”. In the article Goodhart explains:\nSimply put\nGoodhart’s law was originally developed in the context of conducting monetary policy on the basis of targets. However, its application can be observed in many other areas. Here are some other applications of Goodhart’s law that I can think at the top of my head:"
  },
  {
    "objectID": "posts/2024-07-07/index.html#my-fitness-journey",
    "href": "posts/2024-07-07/index.html#my-fitness-journey",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "My Fitness Journey",
    "text": "My Fitness Journey\nAfter hearing about Goodhart’s law, it got me thinking about my own fitness journey. When I was just starting out, I used to get so fixated on the number on weighing scale because in my head I rationalized it as:\nLower weight on the weighing scale = Being closer to my fitness goals\nI became so obsessed with the number that I saw on the weighing scale everyday that I started ‘gaming’ the system. I started pulling off some shenanigans like being over zealous with my cardio, being extremely restrictive with my calories so that the next day the scale would read the number that I wanted to see. It got to a point where I lost track of my broader fitness goals and why I was doing it in the first place - to stay fit and be more healthy.\nIt wasn’t until many years later that I realized that I was doing it all wrong. It occurred me to that weight is one of the many metrics that can be used to gauge your progress. It can serve as a proxy for your fitness journey but it is not the end-all metric that is worth obsessing over. Not only that, but relying solely on a weight loss scale for fitness progress can be misleading. Turns out many variables affect your body weight, such as:\n\nSleep\nCarbohydrate intake\nSodium intake\nFiber\nMenstrual cycle\nFood volume\nConstipation\nStress\nHydration\n\nTherefore even if you are giving 100%, the numbers on weighing scale can fluctuate easily."
  },
  {
    "objectID": "posts/2024-07-07/index.html#ai-goals",
    "href": "posts/2024-07-07/index.html#ai-goals",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "AI Goals",
    "text": "AI Goals\nIn Life 3.0 (which is easily one of my favorite books of all time), Max Tegmark mentions that in our quest to building Artificial General Intelligence (AGI) as we get more intelligent and powerful machines, it will become paramount to ensure that their goals are aligned with ours. He argues that figuring out how to align the goals of a superintelligent AI with our goals isn’t just important, but also hard and continues to be an unsolved problem. For example, if I ask a self-driving car to take me to the airport as fast as possible and it literally takes my word for it, the next thing I know the car is ignoring the speed limits and all traffic signals, I am being chased by helicopters and by the time I arrive at the airport, I find myself covered in vomit. I could get mad at the car all day long and tell it ‘That’s not what I wanted!’, it can argue ‘That’s what you asked for.’"
  },
  {
    "objectID": "posts/2024-07-07/index.html#sustainability-targets-and-initiatives",
    "href": "posts/2024-07-07/index.html#sustainability-targets-and-initiatives",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Sustainability Targets and Initiatives",
    "text": "Sustainability Targets and Initiatives\nLet’s take a look at another example of where Goodhart’s law maybe applicable - sustainability targets and initiatives.\nFollowing the Paris Agreement, the Intergovernmental Panel on Climate Change published its Special Report on Global Warming of 1.5 °C (SR15) which stated that\n\n“Reaching and sustaining net zero global anthropogenic [human-caused] CO2 emissions and declining net non-CO2radiative forcing would halt anthropogenic global warming on multi-decadal timescales.\n\nEver since then, there has been significant growth in the number of actors pledging net zero emissions. Many standards such as the Science Based Targets initiative (SBTi) and International Sustainability Standards have emerged that interpret the net zero concept and aim to measure progress towards net zero targets. The rise of these standards have led to the (re)emergence of various policies and mechanisms such as carbon offsets.\nThe idea behind a carbon offset is that when an entity releases greenhouse gases, they can pay someone else to remove an equivalent amount of climate pollution from the atmosphere. For example, if an automotive manufacturer in a developed country that wants to claim it is reducing its emissions, the manufacturer can pay for a patch of rainforest to be protected in the Amazon that can absorb an equal amount of emissions. This in theory – cancels out - or offsets the impact of the automotive manufacturer’s emissions.\nOne can see the appeal for carbon offsets. For many industries, the cost of addressing their own emissions can be a huge undertaking. By directly funding offset projects, the cost of addressing climate change becomes more manageable. It sounds pretty straightforward, right?\nIt is all fun and games until you take a look at the reality.\nHow it started:\n\n\n\nSource: Marketplace.org\n\n\nHow it’s going:\n\n\n\nSource: Bloomberg.org\n\n\nCarbon offsets have been there since 1970s. So what went wrong along the way?\nKeeping aside the limitations of carbon offsets such as overstated baselines, double counting of emissions, additionality; one can see how Goodhart’s law might be at play here.\nConsider a simple example. A dairy farm wants to setup its operations in California. This means the dairy farm will have to clear 1 hectare of land to be up and running. Theoretically, the dairy farm could offset its impact by planting 1 hectare of vegetation in Amazon. Soon enough, this will prompt other dairy farms to do the same thing and before you know it hundreds of acres of lands have been cleared in California. But you might say : ‘Aren’t they making up for their impacts by offsetting their emissions in Amazon?’\nPartially yes. But 1 hectare of land in California is not the same as 1 hectare of land in Amazon. This has been one of the criticisms of carbon offsets where offset projects tend to encourage ‘business as usual’ practices without considering the ecological complexities of the different regions. As a result, organizations can claim to be carbon neutral (which on paper they might be). But in the end, are we really ‘solving’ for the underlying problem?"
  },
  {
    "objectID": "posts/2024-07-07/index.html#biodiversity-conservation",
    "href": "posts/2024-07-07/index.html#biodiversity-conservation",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Biodiversity Conservation",
    "text": "Biodiversity Conservation\nSimilarly, Goodhart’s law finds it way in a related area - protecting biodiversity.\nRapid declines in populations of various vulnerable species have led to their inclusion in direct measures of biodiversity such as International Union for Conservation of Nature (IUCN), Red List Index (RLI). The Index is based on the IUCN Red List of Threatened Species™, which is widely considered to be the leading assessment of the extinction risk of species. The Red List involves the application of quantitative criteria based on population size, distribution area, and rate of decline, to assign species to different categories of relative extinction risk.\nA substantial research effort has been devoted to examining the causes of decline in these vulnerable species using the indicators used in RLI. This has led to increases in conservation efforts which have had some success in slowing or even reversing the observed declines. At first glance, this can viewed as a success story. However, it has also potentially undermined the use of these species as a sustainability indicator. As policy and management interventions have focused specifically on vulnerable species, with the aim of improving this indicator, abundance of these species is arguably now less representative of the general state of the natural environment than it was hitherto. Consequently, any increase in the indicator is more likely to be a measure of specific response actions than of any general improvement in the state of the environment."
  },
  {
    "objectID": "posts/2024-07-07/index.html#way-forward",
    "href": "posts/2024-07-07/index.html#way-forward",
    "title": "Goodhart’s Law - When Chasing Measures Goes Wrong",
    "section": "Way Forward",
    "text": "Way Forward\nThese examples illustrate the central theme of Goodhart’s law, that is\n\nOnce a measure is declared as important and policy aims to reduce it, the underlying correlation will be reduced.\n\nThis post is not meant to be a criticism of carbon offsets, sustainability standards or measures as a whole. As someone who is part of the ‘what gets measured, gets managed’ crowd, I still believe that measures serve an important purpose.\nBut given what we know about Goodhart’s law, two key questions emerge:\n\nWhy does it happen?\nWhat can we do about it?\n\nIn the episode, Jeremie explains this happens because you end up baking in some misalignment between what you want and what the system wants. The more powerful that system becomes the the more it exploits that gap. People will tend to affect any given indicator/measure in whichever ways can be most readily achieved. As a result, indicators become decoupled from underlying process that they are supposed to indicate, and indicator values will become artificially inflated without addressing the underlying problem.\nThat takes care of the ‘why’. Now how do we actually address it?\nBefore coming up with ways to quantify something, we should be really clear with what are we trying to solve for? Do I want to track my daily weight or do I want to be the healthiest version of myself? Do I want to offset my emissions or do I want to make sure the global ecosystems are protected?\nOnce we are clear on the problem we are trying to solve, we should then think of all the possible ways in which the given indicator/measure could be exploited/gamified. How could our measure of choice become decoupled from the underlying process?\nSometimes we won’t have clear answers to to these questions, especially when we are trying to solve a novel problem where there has been no precedence. However, considering some of the confounding variables at the onset of the process could help us ensure that our measures are aligned with what we are trying to solve for.\nFinally, systems should be put in place to prevent manipulation of the indicators and the assessments on which they are based, to ensure that the information they provide is objective and reliable. Using multiple indicator sets, including measures of pressure as well as state variables, could help reduce scope for indicator manipulation.\nIn conclusion, measures serve an important function in guiding decision-making and assessing progress toward goals. However, there is a need for caution in interpreting the information provided by any measure that are used; at best, they can only provide a partial indication of the status and trends of the process we are trying to assess, and this needs to be appreciated by the decision-makers who employ them."
  },
  {
    "objectID": "posts/2024-07-15/index.html",
    "href": "posts/2024-07-15/index.html",
    "title": "AI and Sustainability - How to make them coexist?",
    "section": "",
    "text": "AI and Sustainability - two of the hottest topics right now. From regulators to policymakers, businesses, research institutions, non-governmental organizations (NGOs), academia, investors, consumers, and communities at large, everyone wants to see what does AI have to offer while at the same time Earth is witnessing record temperatures.\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious and I decided to do a deep dive to explore the interplay between these two areas and address the question ‘how do we make them compatible?’\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. One also needs to factor in the additional footprint of hardware manufacturing, transportation, infrastructure overheads as well the water required for running these models.\n\n\n\nSource: Strubell et al.\n\n\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company; all these figures shouldn’t come as a shocker.\nA surge in demand for the data centers required for artificial intelligence is having an impact on Big Tech’s carbon emissions. Google recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment.\n\nMicrosoft is in the same boat as well. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for them.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient? Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was “I agree”. AI is already showing promising results in multiple areas and some of these climate solutions are just tip of the iceberg - and they are only going to get better.\nNow it becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nHowever despite all this evidence, something just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThe idea that AI benefits will offset the environmental costs is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient. But what if it is the other way round? What if the environmental costs of running these systems outweigh the benefits they have to offer?\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there is still room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption. Many worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done?More importantly can we develop these solutions before we surpass the planetary boundaries?\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameter language model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negligible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’, Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models\nDeveloping tools for accurate energy estimation\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Processing (NLP) by:\n\nIncreasing the alignment between experiments and research hypotheses\nEncouraging the release of trained models\nSetting up tracks that target efficiency\n\nThese are just two examples that highlight the good work that is being done to address this problem from both sides. This is where approaching this problem through the lens of ‘double materiality’ might be useful - where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from both sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?"
  },
  {
    "objectID": "posts/2024-07-15/index.html#context",
    "href": "posts/2024-07-15/index.html#context",
    "title": "AI and Sustainability - Can They Coexist?",
    "section": "Context",
    "text": "Context\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious to explore these two seemingly ‘incompatible’ areas and address the question ‘how do we make them compatible?’\nBesides the risks of climate change and AI, we have risks caused by the interaction of these risks i.e .risks on AI due to climate change and risks on climate change due to AI. This is a perfect example of a ‘polycrisis’, a concept that can be tracked back to 1999 when Edgar Morin and Anne Brigitte Kern used it to describe “interwoven and overlapping crises” in their book Homeland Earth: A Manifesto for a New Millennium. In theory, managing multiple risks in isolation is feasible. However it gets a lot more complicated when you have two risks interacting with each other."
  },
  {
    "objectID": "posts/2024-07-15/index.html#quick-facts",
    "href": "posts/2024-07-15/index.html#quick-facts",
    "title": "AI and Sustainability - Can They Coexist?",
    "section": "Quick Facts",
    "text": "Quick Facts\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. Not only that these AI models can have an impact on water use.\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company, all these figures shouldn’t come as a shocker.\nGoogle recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, andthe emissions associated with the expected increases in our technical infrastructure investment.\n\nFor Microsoft, it is a similar theme. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report published Wednesday.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for the Seattle-based tech giant.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient. Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was ‘’I agree’’. AI is already showing promising results in multiple areas and is even outperforming humans in climate solutions. These applications are just tip of the iceberg. And they are only going to get better.\nIt becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nDespite all this evidence, some thing just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThis is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient.\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there still remain room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which was a breakthrough and highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used to make even better AI systems.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption.\nMany worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems. This further complicates the problem.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done More importantly can we develop these solutions before we surpass the planetary boundaries?\nDo we curb the use of AI altogether? The problem is not use of AI per se. But rather figuring out creative solutions to make them more compatible.\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameterlanguage model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negliglible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’ , Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models.\nDeveloping tools for accurate energy estimation.\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources:\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Prcoessing (NLP) by reducing the computational costs of model training. These recommendations include:\n\nIncreasing the alignment between experiments and research hypotheses.\nEncouraging the release of trained models.\nSetting up tracks that target efficiency\n\nThese are just some of recommendations that highlight the good work that is being done to make AI and Sustainability ‘compatible’ with each other.\nThis is where approaching this problem through the lens of ‘double materiality’ might be useful. Where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?\n\nThe question is not whether ChatGPT is bad\napart from figuring out the new use cases for where AI could be useful, they now also have to think about how could we mitigate some of the negative effects of large scale AI use?\nQuestion becomes ’’Can we trust AI to within narrow window of opportunity?\nPolicy making, corporate governance, community engagement, technological innovation, and international cooperation are equally vital components in our collective toolkit for addressing sustainability challenges. Each plays a crucial role in shaping how AI and other technologies are developed, regulated, and deployed to achieve long-term environmental and social goals.\nnot only could AI reduce road fatalities,\nThat’s the fundamental ethos behind ‘Utilitrainimsim’.\nhttps://www.utilitarianism.com/jsmill-utilitarianism.pdf\nSome thing just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if me using ChatGPT is indeed causing degradation of global ecosystems?’\nthought to myself and said ‘’it does make sense’‘. to an extent. A quick search of ’AI use cases in sustainability’ and you will find hundreds, if not thousands of such applications. But then I thought to myself, this assumption lies on the fact AI systems will get more efficient.\nOnce technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. The cost of information technology has now halved roughly every two years for about a century, enabling the information age.\nMy concern is not whether they are compatible or not? A more pressing concern in my opinion is how can we make them work together in a way\nSo what’s wrong? If AI is showing promise in addressing these problems are we being too paranoid about it?\nGlobal planetary boundaries. before time runs out.\nWhen we had a panel discussion about this in Austin, Texas, at the 2016 annual meeting of the Association for the Advancement of Artificial Intelligence, the Ihe exclaimed. Because almost all car crashes are caused by human error, it’s widely believed that AI-powered self-driving cars can eliminate at least 90% of road deaths, and this optimism is fueling great progress toward actually getting self-driving cars out on the roads.\nAI can assist in the identification of areas that are at high risk of climate-related hazards, the development of adaptation strategies for businesses and communities, the prediction of floods and wildfires, and the identification of areas at risk of landslides (Rutenberg et al., 2021). Furthermore, AI-powered climate modeling and projection systems can help policymakers and stakeholders anticipate the impacts of climate change and design effective mitigation and adaptation strategies. AI can also help in the development of early warning systems that can alert communities to impending disasters, providing them with crucial time to prepare and evacuate.\nthe net balance in terms of climate is that AI will be a very good\nthat extra 5% of demand is not the thing that prevents our climate goals. thing, whether it’s the material science part of the thing or managing plasmas infusion reactors, it’s going to accelerate the innovation.\nHis main argument is that\nAI will ‘pay for itself’ when it comes to its associated environmental costs. His main argument is that AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters.\nthe n\nMaybe there is less doom and gloom?\nWe all know people around the world are exploring the use cases of AI within sustainability space.\nBill Gates recently said by enabling the development of innovative strategies for climate change adaptation. AI can assist in the identification of areas that are at high risk of climate-related hazards, the development of adaptation strategies for businesses and communities, the prediction of floods and wildfires, and the identification of areas at risk of landslides (Rutenberg et al., 2021). Furthermore, AI-powered climate modeling and projection systems can help policymakers and stakeholders anticipate the impacts of climate change and design effective mitigation and adaptation strategies. AI can also help in the development of early warning systems that can alert communities to impending disasters, providing them with crucial time to prepare and evacuate.\nI can see his point totally. One of the use cases that I was excited to see was CLIMATEBERT, a transformer-based language model developed by Webersinke et al. (2021) for analysing climate-related text.\nBesides, being pleasantly surprised by the I was very pleased to see a section called ‘Carbon Footprint’\nBut turns out that didn’t result in the consumption of less electricity. On the contrary, it resulted in the same amount of electricity being used to make even better AI systems. In economics, this phenomenon is commonly referred to as ‘Jevon’s paradox’ , named after Jevon who noted that the improvement of steam engine by James Watt which allowed for much less coal to be used, instead led to a huge increase in the amount of the fossil fuel burned in England.\nThe concern here is we wouldn’t want the same thing to happen with banking our trust in AI. We are hoping that AI will help us reduce our emissions by optimizing our\nSo what’s the way forward?\nBut are we betting too much on AI to save the world for us?\nWebersinke, N., Kraus, M., Bingler, J. A., & Leippold, M. (2021). Climatebert: A pretrained language model for climate-related text. arXiv preprint arXiv:2110.12010.\nhttps://www.bloomberg.com/news/articles/2024-06-26/why-bill-gates-is-bullish-on-artificial-intelligence-and-nuclear-energy\nIt makes me wonder\nWhen you pair these figures with\nAssuming a continued annual relative growth ranging from 5.6 to 6.9%, ICT’s relative contribution would exceed 14% of the 2016-level worldwide GHGE by 2040 […]\nWhen you pair these figures with\nWhen almost makes me question - maybe reactionary claims true\nAnd as the world’s demand for such AI technology continues to grow, so does the AI industry’s energy consumption. In an environmentally hostile chain reaction, rapidly increasing computational needs will unavoidably escalate carbon costs.\nThe data, published in Amazon’s annual sustainability report on Wednesday, contrast with disclosures from rivals Microsoft Corp. and Alphabet Inc.’s Google, which are struggling to reduce their emissions amid a surge in demand for the data centers required for artificial intelligence.\nAs datasets and models become more complex, the energy needed to train and run AI models becomes enormous. This increase in energy use directly affects greenhouse gas emissions, aggravating climate change. According to OpenAI researchers, since 2012, the amount of computing power required to train cutting-edge AI models has doubled every 3.4 months. By 2040, it is expected that the emissions from the Information and Communications Technology (ICT) industry as a whole will reach 14% of the global emissions, with the majority of those emissions coming from the ICT infrastructure, particularly data centres and communication networks. These data demonstrate the urgent need to address AI’s carbon footprint and role in environmental deterioration.\nRecently, a study was conducted by researchers at the University of Massachusetts to determine how much energy is used to train certain popular large AI models. According to the results, training can produce about 626,000 pounds of carbon dioxide, or the equivalent of around 300 round-trip flights between New York and San Francisco – nearly 5 times the lifetime emissions of the average car.\nWhat is the way forward?\nIt’s one of those things where you know there is a promise but at the same is it going to be worth it? A looming thought emerges - what\nIt is a very interesting read and I would highly encourage everyone to take a look at it.\nYou have AI and Sustainability\nThere is a lot of talk\nhttps://arxiv.org/abs/1906.02243 - Training a model emits 5 times the emissions\nBloom Model - https://cifar.ca/wp-content/uploads/2023/09/Towards-Measuring-and-Mitigating-the-Environmental-Impacts-of-Large-Language-Models.pdf\nCase Study: The Challenge with Generative AI and Large Language Models The AI carbon footprint remains of high concern as AI continues to increase in use and popularity. According to Open AI researchers, since 2012, the computing power required to train cutting-edge AI models has doubled every 3.4 months. By 2040, it is expected that the emissions from the Information and Communications Technology (ICT) industry will reach 14% of the global emissions, with the majority of those emissions coming from the ICT infrastructure, particularly data centers and communications networks.”12 Training a single large language model (LLM) like ChatGPT can emit up to 626,155 tons of CO2e, which is more than the lifetime emissions of five average American cars, including fuel.13 This is because LLMs require massive amounts of data and computing power, which consume a lot of electricity from the grid, often generated by fossil fuels. The GHG emission, energy consumption, and water use of training generative AI models like large language models can be significant. For example, some estimates suggested that training a single large language model can use around 300 to 500 tons of CO2, about 60 times the average person’s annual carbon footprint. 14 This is a significant amount of energy, especially considering that many AI models are trained on large datasets and require a lot of computing power. A separate study also estimates that large data centers used to train and run these large AI models can consume 10s of billions of liters of fresh water.15 This does not even include the footprint of hardware manufacturing, transportation, and infrastructure overheads.\nBERT excerpt about AI\n’’Nevertheless, we decided to carry out this project, as we see the high potential of NLP to support action against climate change. Given our awareness of the carbon footprint of our research, we address this sensitivetopic as follows:\nI would argue its not only an for AI question as well.\nWe can’t rely on historical data for prediciting extreme events in the future.\nNot only that,\nCreative solutions will be needed. - Jevon’s Paradox\nhttps://www.theguardian.com/business/article/2024/jul/04/can-the-climate-survive-the-insatiable-energy-demands-of-the-ai-arms-race\nBill GAtes\nIEA\nChatGPT\nhttps://www.bloomberg.com/news/articles/2024-07-02/google-s-emissions-shot-up-48-over-five-years-due-to-ai\nhttps://www.bloomberg.com/news/articles/2024-06-26/bill-gates-says-ai-will-advance-green-tech-offsetting-its-emissions\ncan we really use AI for future weather impacts?\nnot really. Historical trends do not correspond to future.\nMaybe anamoly detection.\nUnlike traditional supervised learning applications, it’s not as simple as\nTalk about pros and cons from here: https://link.springer.com/article/10.1007/s43762-023-00100-2\nWe can’t use historical data to predict future climate risk\nhttps://www.popsci.com/story/environment/underestimating-extreme-weather-climate-change/\nhttps://www.science.org/doi/10.1126/sciadv.aay2368\nhttps://www.hoover.org/research/flawed-climate-models\nhttps://20965052.fs1.hubspotusercontent-na1.net/hubfs/20965052/AIs%20Impact%20on%20Our%20Sustainable%20Future%20White%20Paper%20V1.pdf\nI think its a unique challenge for both domains. For Sustainability - apart from figuring out the new use cases for where AI could be useful, they now also have to think about how could we mitigate some of the negative effects of large scale AI use?\nFor the tech folks, the question becomes how do we make better algorithms, efficiency etc.brainstorm how can we better address these issues\nI am going to borrow the concept of dual materiality and say the question\nHow does AI have an impact on sustainability and vice versa?"
  },
  {
    "objectID": "posts/2024-07-21/index.html",
    "href": "posts/2024-07-21/index.html",
    "title": "AI and Sustainability - How to make them coexist?",
    "section": "",
    "text": "AI and Sustainability - two of the hottest topics right now. From regulators to policymakers, businesses, research institutions, non-governmental organizations (NGOs), academia, investors, consumers, and communities at large, everyone wants to see what does AI have to offer while at the same time Earth is witnessing record temperatures.\nThe other day I saw a headline that read something along the lines of ‘STOP USING ChatGPT BECAUSE IT IS BAD FOR THE ENVIRONMENT!!!’.\nNaturally, it got me curious and I decided to do a deep dive to explore the interplay between these two areas and address the question ‘how do we make them compatible?’\nIn Electricity 2024, IEA reported that data center electricity usage is set to double by 2026 due to the rise of power-intensive workloads such as AI and cryptocurrency mining.\n\n\n\nSource: IEA - Electricity 2024 - Analysis and Forecast to 2026\n\n\nWithin the data center segment, IEA wrote that computing power and cooling were the two most energy-intensive processes within data centers. Additionally, the report noted how the rapid growth of artificial intelligence-related services over the last 12 months means providers have invested in power-hungry GPUs.\nAccording to OpenAI, the amount of computing power used for deep learning research since 2012 has been doubling every 3.4 months. In a 2018 study, researchers at the University of Massachusetts, Amherst, performed a life cycle assessment for training several common large AI models and found that training these large AI models can emit more than 626,000 pounds of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. One also needs to factor in the additional footprint of hardware manufacturing, transportation, infrastructure overheads as well the water required for running these models.\n\n\n\nSource: Strubell et al.\n\n\nIn an age where there semiconductors are becoming a hot commodity, NVIDIA being the world’s most valuable publicly traded company; all these figures shouldn’t come as a shocker.\nA surge in demand for the data centers required for artificial intelligence is having an impact on Big Tech’s carbon emissions. Google recently released its Environmental Report. The report states:\n\nIn 2023, our total GHG emissions were 14.3 million tCO2e, representing a 13% year-over- year increase and a 48% increase compared to our 2019 target base year. This result was primarily due to increases in data center energy consumption and supply chain emissions. As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment.\n\nMicrosoft is in the same boat as well. The Seattle-based company’s total planet-warming impact is about 30% higher today than it was in 2020, according to the latest sustainability report.\nInterestingly enough, Amazon’s GHG emissions dropped by 3% in 2023. Amazon has pledged to spend more than $150 billion on data centers in the next 15 years. It would be interesting to see how it all plays out for them.\nBut maybe these emissions will offset themselves in a couple of years as AI systems get more efficient? Atleast, that’s what Bill Gates has to say on this matter. He believes that AI will ‘pay for itself’ when it comes to its associated environmental costs and AI is already being used to solve some of the pressing environmental problems such as optimizing energy demand, developing early warning systems for natural disasters, development of adaptation strategies for businesses and communties. So according to Gates, that ‘extra 5%’ of energy demand is not the thing that prevents our climate goals.\nWhen I heard that interview, the first thought I had was “I agree”. AI is already showing promising results in multiple areas and some of these climate solutions are just tip of the iceberg - and they are only going to get better.\nNow it becomes a moral question : ‘Given the promising solutions that AI has to offer for addressing some of the pressing problems, why would we NOT want to continue to use AI?’\nIt’s the same rationale computer scientist Moshe Vardi has when it comes to developing self driving cars. He views it as a moral imperative given how many lives could be saved by automated driving. Going back to Ethics 101, this moral imperative stems from the ethical theory of Utilitarianism where the morally right action is the one that produces the greatest amount of happiness or pleasure (and the least amount of pain or suffering) for the largest number of people. If we apply this utilitarian approach to the case of AI and Sustainability, it is in our best interests to atleast consider that AI does offer a path to a sustainable future.\nHowever despite all this evidence, something just doesn’t sit right with me. There is still a looming thought in my head ‘What if those reactionary headlines that I read are true? What if using ChatGPT is indeed causing degradation of global ecosystems?’\nThe idea that AI benefits will offset the environmental costs is based on the assumption that AI developments will continue to grow and with the passage of time AI systems will get more efficient. But what if it is the other way round? What if the environmental costs of running these systems outweigh the benefits they have to offer?\nHistorically, the rate of improvement of computing technology has been described by the famous Moore’s law, which in one of its variations states that computing power per dollar doubles every 18 months or so. Yes, there are physical limits to Moore’s law but there is still room for advances in computing technology. As Max Tegmark points out in Life 3.0, once technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. We are making breakthroughs in AI that enables us to do more with less. Consider DeepMind’s Chinchilla which highlighted that AI models could be using radically less computing power, by changing the ratio between the amount of training data and the size of the resulting model. One would assume that this efficiency would translate into AI systems using less electricity. Turns out that was not the case and it resulted in the same amount of electricity being used.\nThis is a classic example of a phenomenon referred to as “Jevons’ paradox”, named after the English economist William Stanley Jevons who noted who noted that improvements that increased the efficiency of coal use such as the James Watt steam engine led to increased consumption of fossil fuel burned in England during the Industrial Revolution. As a result, he argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption. Many worry that in our quest to develop AI systems to solve , something similar may play out - where better AI systems end up driving the demand for resources rather than a decrease in resource consumption. This means more electricity consumed to fuel these systems.\nThe concern now becomes - can these AI systems that are being developed to address the sustainability problems end up backfiring and undo all the hard work that we have done?More importantly can we develop these solutions before we surpass the planetary boundaries?\nFortunately, these two seemingly incompatible domains can be made more compatible. Recent research by Luccioni et al. (2022) on the carbon footprint of BLOOM, a 176-billion parameter language model, showed that both the manufacturing of equipment, model training and deployment can be carried out in a way that results in negligible amounts of carbon emissions when compared to other similar size models.\nIn a policy brief titled ‘TOWARDS MEASURING AND MITIGATING THE ENVIRONMENTAL IMPACTS OF LARGE LANGUAGE MODELS’, Dr. Luccioni further highlights some recommendations for responsible LLM innovation with sustainability in mind:\n\nCreating standards and frameworks for evaluating and reporting the carbon footprint of large language models\nDeveloping tools for accurate energy estimation\nMandates for environmental impact with the release of LLM-based systems\nThe creation of certification and ratings of AI models\nCarbon-aware model training\nExpanding renewable energy resources\n\nSimilarly, Association of Computational Logistics (ACL) outlined recommendations to address some of the environmental concerns in Natural Language Processing (NLP) by:\n\nIncreasing the alignment between experiments and research hypotheses\nEncouraging the release of trained models\nSetting up tracks that target efficiency\n\nThese are just two examples that highlight the good work that is being done to address this problem from both sides. This is where approaching this problem through the lens of ‘double materiality’ might be useful - where we consider not just the impact of AI on sustainability but also the impact of sustainability on AI.\nThe key takeaway is addressing this problem will require creative solutions from both sides. The AI community will need to figure out how can they make these systems that minimize environmental impact. On the flip side, governments, policymakers, researchers, businesses, non-profits will need to brainstorm new uses cases for AI in solving some of the sustainability problems which includes negative impacts of large scale AI deployment.\nAt the end of the day, we need to realize that AI is just one of the many tools in our arsenal for addressing the sustainability challenges we face. Policy making, strong governance, and technological innovation are all essential components for achieving our ambitious goals.\nThe question is not whether ChatGPT is good/bad for the environment. A broader question that we should instead focus on:\n\nCan we figure out a way to save our planet using AI within this narrow window of opportunity?"
  },
  {
    "objectID": "posts/2024-08-04/index.html",
    "href": "posts/2024-08-04/index.html",
    "title": "AI Washing - Is it time to put a brake on the AI Hype Train?",
    "section": "",
    "text": "If you are anything like me, every day you wake up to see something new happening in the AI space. Whether its is OpenAI creating its own search engine SearchGPT or Meta releasing Llama 3.1, AI stuff is everywhere. I am not going to lie, reading about the latest and greatest happening in the AI ecosystem is really exciting for me. The other day I read about Adobe coming up with new audio editing features that removes instances of filler words such as ‘umh’. Granted it is not perfect, but it is only a matter of time before it becomes even better.\nHowever, it is getting to the point where I am finding it too much. These days it is almost impossible for me to browse my newsfeed without ever encountering the term ‘GenAI’.\nMy honest reaction whenever I see a new 'GenAI' related post on my newsfeedvia GIPHY\nCurating my newsfeed has definitely helped. However, I asked myself: \"Are we really going overboard with AI?\" Don’t get me wrong, I am not an AI skeptic in any shape or form. On the contrary, I am excited to see how it all plays out. But maybe we need to be realistic with our expectations about AI? Big Tech’s latest earnings suggest that there isn’t much revenue to show for the AI hype train. Granted that could change in the future, but as Julia Angwin describes in her article, \"some of A.I.’s greatest accomplishments seem inflated\" and it will take a while for AI to live up to its hype of becoming the most powerful technology humanity has yet invented.\nWhat could be possibly causing this? My theory is that this misalignment between the hype and the actual reality of AI comes down to the vagueness of the term ‘artificial intelligence’ itself.\nThe term artificial intelligence has been around since 1956 when John McCarthy, coined it during a conference at Dartmouth College. The proposal stated:\n\nThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\nHow can it be that the concept of AI has been around since decades and yet there is no universally agreed upon definition of ‘artificial intelligence’ ?\nThat’s because there is no definition of the term ‘intelligence’ itself. In Life 3.0, Max Tegmark defines intelligence as the ability to accomplish complex goals. And because there are many possible goals, there are many possible types of intelligence. Every now and then I keep reading about feats of intelligence in animal kingdom from ants being able to collectively solve complex tasks to dolphins being able to sense electric currents in water (electroreception) to avoid predators, find food, navigate or even communicate with one another. Clearly they are ‘intelligent’ in their own ways, it’s just that we don’t know how to measure that intelligence. Therefore, according to Tegmark, it makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ.\nThat isn’t to say there haven’t been attempts to capture the essence of AI. AI has its roots in statistics and machine learning and can be traced back to works of Alan Turing and Arthur Samuel in 20th Century. These researchers were interested in having machines learn from data and attempted to approach the problem through generalized linear models and probabilistic reasoning. Eventually, these methods evolved and led to some early developments in the broad AI space. Somewhere along the way, the boundaries between traditional statistical methods, machine learning and artificial intelligence got blurred. Fast forward to today and the meme of ‘It’s AI in the sales pitch and machine learning in the prototype’ is now a reality.\n\n\n\n\nSource: Sandserif Comics\n\n\n\n\n\nThe power of the hype. Source: Wallace Ferreira\n\n\nJokes aside, this isn’t to say all Statistics = Machine Learning = Artificial Intelligence. Joe Davison covers this in the article ‘No, Machine Learning is not just glorified Statistics’ and sums it up succinctly : ‘’reducing machine learning as a whole to nothing more than a subsidiary of statistics is quite a stretch.’’\nInstead the way to look at it is that artificial intelligence is this broad umbrella category with machine learning being a subset of artificial intelligence and deep learning in turn being a subset of machine learning which ultimately has roots in statistics.\n\n\n\nAn illustration of the position of deep learning (DL), comparing with machine learning (ML) and artificial intelligence (AI). Source: Sarker (2021)\n\n\nThe vagueness of terms like AI starts emerging when people start conflating these terms with one another. Eric Siegel, former professor at Columbia University talks about this extensively in his work. According to Siegel,\n\nMost people conceive of ML as “AI.” This is a reasonable misunderstanding. But “AI” suffers from an unrelenting, incurable case of vagueness — it is a catch-all term of art that does not consistently refer to any particular method or value proposition. Calling ML tools “AI” oversells what most ML business deployments actually do. In fact, you couldn’t overpromise more than you do when you call something “AI.” The moniker invokes the notion of artificial general intelligence (AGI), software capable of any intellectual task humans can do.\n\nHe further talks about a paradox, known as The AI Effect where either the definition of AI or the concept of intelligence is adjusted to exclude capabilities that AI systems have mastered. In other words, if everything is AI, then nothing is truly AI.\nIf you think about it, this problem is not unique to the AI space. Sustainability space continues to be afflicted by what constitutes as ‘sustainable’? No wonder there is a huge focus on addressing greenwashing risks to seperate the facts from the fiction and cut down misleading environmental claims.\nSo going back to the title of the post, is it time to borrow a page from the sustainability space and address ‘AI washing’, investments directed towards projects labeled as AI without rigorous scrutiny of their actual impact?\nDo we really need to have ‘AI-features’ in our electric toothbrushes (unless it’s going to save me a visit to my dentist every 6 months or so, then I am all ears)? Or how about an ‘AI Button’ on your mouse and your keyboard? (I mean, really?)\nNow I am not implying that these features aren’t welcome and the we all should be become AI Luddites. I would be lying if I said the thought of trying these features didn’t occur to me. But this raises some questions such as \"Do I really need to pay $100 extra for an ‘AI-powered toothbrush’ that is pretty much going to do the same thing?\" or more broadly \"Do we really need to slap the term GenAI to something that would had been routine ‘bug fixes’ or ‘feature enhancements’ six months ago?\"\nFortunately, there is work being done to address AI washing. Last year, the Federal Trade Commission (FTC) warned companies across the economy that it would be on the lookout for false AI claims in advertising, Similarly, Securities and Exchange Commission (SEC) Chair Gary Gensler has warned businesses against making exaggerations of AI’s capabilities. Earlier this year, the SEC imposed the first civil penalties on two companies, Delphia Inc and Global Predictions Inc, for misleading statements about their use of AI. In Europe, the Artificial Intelligence Act was approved in May 2024 with the goal of harmonising rules on AI.\nLove it or hate it, I still think that AI is here to stay. Ultimately, the future of AI is one of the most important conversations we need to have. Maybe our goal shouldn’t be jumping off the AI train but rather building more transparent and responsible AI systems that facilitate open and honest discourse."
  },
  {
    "objectID": "posts/2024-08-17/index.html",
    "href": "posts/2024-08-17/index.html",
    "title": "Is AI an ‘existential risk’?",
    "section": "",
    "text": "From Terminator to Westworld, almost all depictions of AI that I have seen in media follow a similar theme: a malevolent entity that aims to subjugate human beings. In fact, I am having a hard time recalling any major piece of media that isn’t set in the dystopian future and depicts humans and AI living in harmony. If you have any good recommendations of a movie, TV series, novel or any other major piece of art that depicts humans and AI having a jolly good time, do let me know.\nBut coming to the concept of AI being an existential threat. How did I stumble across this idea?\nThe answer is this comprehensive article published on 80000 hours. I would definitely recommend giving it a read because it outlines some really good points about the future of AI. In fact, risks from artificial intelligence are ranked as the number one in the world’s most pressing problems above catastrophic pandemics, nuclear weapons, power war and climate change. On March 22, 2023, thousands of tech leaders, researchers and AI experts including Elon Musk, Stuart Russell, Max Tegmark signed an open letter urging a moratorium on the development of the most powerful artificial intelligence systems. Almost two months later, a similar letter titled ‘Statement on AI Risk’ was signed by another group of world’s leading AI scientists and experts.\nA natural question to ask would be: why is this all happening of a sudden?\nThe number of AI-lobbying organizations in the U.S. spiked from 158 in 2022 to 450 in 2023. That marks a whopping 185% increase in AI lobbying with the goal of regulating AI. There have been reports suggesting that there is in fact a growing network of AI advisers, think tanks and policy groups that are influencing federal agencies on AI policy discourse. For example, a bipartisan bill was passed in the U.S. Senate setting the framework for AI legislation. It would would create a new authority that any company developing AI would have to register to and seek a license from. Similar developments are taking place around the world. According to Stanford University’s 2023 AI Index, the annual number of bills mentioning “artificial intelligence” passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.\nAt a first glance, all these developments look like a step in the right direction. In hands of bad actors, any technology like AI can be a bad thing. We are already seeing a rise in scams, deepfakes, voice cloning, misinformation and cyberattacks.\nHowever, there are many who believe that this narrative of AI being an existential risk is being used to monopolize AI in hands of select few. Andrew Ng, a globally recognized leader in AI, believes that imposing strict licensing requirements and blanket AI regulations is is based on the “bad idea that AI could make us go extinct”. In his view, this is a way for big tech to create regulatory capture to ensure that open source alternatives cannot compete. Regulatory capture is a concept where a regulatory agency enacts policies that favor the industry at the expense of the broader public interest, in this case with regulations that are too onerous or expensive for smaller businesses to meet. This has led to a growing movement to democratize AI governance. Letters such as Joint Statement on AI Safety and Openness and A Call to Protect and Open-Source AI in Europe have been signed by various scientists, policymakers, researchers and other prominent figures in the field of AI.\nSuresh Venkatasubramanian, a professor of computer science at Brown University who co-authored last year’s White House Blueprint for an AI Bill of Rights believes that the focus on cataclysmic AI risk is “speculative science fiction” that borders on “fearmongering.” He argues:\n\n“There’s a push being made that the only thing we should care about is long-term risk because ‘It’s going to take over the world, Terminator, blah blah blah,’”\n“I think it’s important to ask, what is the basis for these claims? What is the likelihood of these claims coming to pass? And how certain are we about all this?”\n\nWhen one sits down to analyse these claims, a natural question to start off would be:\n‘Is superintelligence even possible?’ The answer to that question depends on who you ask. As Nick Bostrom mentions in Superintelligence, expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. According to Stuart Armstrong and Kaj Sotala. predictions about the future development of artificial intelligence, “are as confident as they are diverse.”\nMany surveys have been conducted on this topic where AI experts have been asked about their expectations for the future of machine intelligence (I really like this article and this survey conducted by Nick Bostrom in 2012-2013) and turns out there is huge disagreement between experts about when human-level AI will be developed. Granted there are limitations when interpreting the results of these surveys, but the take home message is that there is a high degree of uncertainty about the future of AI. That does not mean there is no risk involved. But what is that risk exactly? Is it Skynet? Ultron? Or another killer robot?\nIn Life 3.0, Max Tegmark debunks the long standing narrative of AI going rogue:\n\nThe fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.\nTherefore, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.\n\n\n\n\nSource: Can we build AI without losing control over it? | Sam Harris\n\n\nThis raises the question: ’What do we do about these risks of AI and more importantly how do we make sure our goals align with that of a superintelligent AI? In the book, Max mentions that goal alignment remains to be an unsolved problem which would require its own post but broadly speaking it can be further subdivided into three sub-problems:\n\nMaking AI learn our goals\nMaking AI adopt our goals\nMaking AI retain our goals\n\nTo wrap things up, any technology including AI poses a risk. This risk stems from misalignment between our goals and the goal of AI. In my opinion, addressing that risk will require strong policy making and open discourse to facilitate responsible AI development. Clearly there is a need for some regulatory oversight to address these risks. In my last post, I mentioned about there is work being done to address ‘AI Washing’ by regulators like Federal Trade Commission, Securities and Exchange Commission. But a question needs to be asked:\n\nWhat do we want out of these regulations? Do we want to monopolize these developments in the hands of select few or do we truly want responsible AI systems?"
  },
  {
    "objectID": "posts/2024-08-24/index.html",
    "href": "posts/2024-08-24/index.html",
    "title": "Is AI an ‘existential risk’?",
    "section": "",
    "text": "From Terminator to Westworld, almost all depictions of AI that I have seen in media follow a similar theme: a malevolent entity that aims to subjugate human beings. In fact, I am having a hard time recalling any major piece of media that isn’t set in the dystopian future and depicts humans and AI living in harmony. If you have any good recommendations of a movie, TV series, novel or any other major piece of art that depicts humans and AI having a jolly good time, do let me know.\nBut coming to the concept of AI being an existential threat. How did I stumble across this idea?\nThe answer is this comprehensive article published on 80000 hours. I would definitely recommend giving it a read because it outlines some really good points about the future of AI. In fact, risks from artificial intelligence are ranked as the number one in the world’s most pressing problems above catastrophic pandemics, nuclear weapons, power war and climate change. On March 22, 2023, thousands of tech leaders, researchers and AI experts including Elon Musk, Stuart Russell, Max Tegmark signed an open letter urging a moratorium on the development of the most powerful artificial intelligence systems. Almost two months later, a similar letter titled ‘Statement on AI Risk’ was signed by another group of world’s leading AI scientists and experts.\nA natural question to ask would be: why is this all happening of a sudden?\nThe number of AI-lobbying organizations in the U.S. spiked from 158 in 2022 to 450 in 2023. That marks a whopping 185% increase in AI lobbying with the goal of regulating AI. There have been reports suggesting that there is in fact a growing network of AI advisers, think tanks and policy groups that are influencing federal agencies on AI policy discourse. For example, a bipartisan bill was passed in the U.S. Senate setting the framework for AI legislation. It would would create a new authority that any company developing AI would have to register to and seek a license from. Similar developments are taking place around the world. According to Stanford University’s 2023 AI Index, the annual number of bills mentioning “artificial intelligence” passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.\nAt a first glance, all these developments look like a step in the right direction. In hands of bad actors, any technology like AI can be a bad thing. We are already seeing a rise in scams, deepfakes, voice cloning, misinformation and cyberattacks.\nHowever, there are many who believe that this narrative of AI being an existential risk is being used to monopolize AI in hands of select few. Andrew Ng, a globally recognized leader in AI, believes that imposing strict licensing requirements and blanket AI regulations is is based on the “bad idea that AI could make us go extinct”. In his view, this is a way for big tech to create regulatory capture to ensure that open source alternatives cannot compete. Regulatory capture is a concept where a regulatory agency enacts policies that favor the industry at the expense of the broader public interest, in this case with regulations that are too onerous or expensive for smaller businesses to meet. This has led to a growing movement to democratize AI governance. Letters such as Joint Statement on AI Safety and Openness and A Call to Protect and Open-Source AI in Europe have been signed by various scientists, policymakers, researchers and other prominent figures in the field of AI.\nSuresh Venkatasubramanian, a professor of computer science at Brown University who co-authored last year’s White House Blueprint for an AI Bill of Rights believes that the focus on cataclysmic AI risk is “speculative science fiction” that borders on “fearmongering.” He argues:\n\n“There’s a push being made that the only thing we should care about is long-term risk because ‘It’s going to take over the world, Terminator, blah blah blah,’”\n“I think it’s important to ask, what is the basis for these claims? What is the likelihood of these claims coming to pass? And how certain are we about all this?”\n\nWhen one sits down to analyse these claims, a natural question to start off would be:\n‘Is superintelligence even possible?’ The answer to that question depends on who you ask. As Nick Bostrom mentions in Superintelligence, expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. According to Stuart Armstrong and Kaj Sotala. predictions about the future development of artificial intelligence, “are as confident as they are diverse.”\nMany surveys have been conducted on this topic where AI experts have been asked about their expectations for the future of machine intelligence (I really like this article and this survey conducted by Nick Bostrom in 2012-2013) and turns out there is huge disagreement between experts about when human-level AI will be developed. Granted there are limitations when interpreting the results of these surveys, but the take home message is that there is a high degree of uncertainty about the future of AI. That does not mean there is no risk involved. But what is that risk exactly? Is it Skynet? Ultron? Or another killer robot?\nIn Life 3.0, Max Tegmark debunks the long standing narrative of AI going rogue:\n\nThe fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.\nTherefore, the real risk with AGI isn’t malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.\n\n\n\n\nSource: Can we build AI without losing control over it? | Sam Harris\n\n\nThis raises the question: ’What do we do about these risks of AI and more importantly how do we make sure our goals align with that of a superintelligent AI? In the book, Max mentions that goal alignment remains to be an unsolved problem which would require its own post but broadly speaking it can be further subdivided into three sub-problems:\n\nMaking AI learn our goals\nMaking AI adopt our goals\nMaking AI retain our goals\n\nTo wrap things up, any technology including AI poses a risk. This risk stems from misalignment between our goals and the goal of AI. However, we also need to be careful that we are not overselling these risks. In my last post, I talked about how ‘AI Washing’ stems from overselling the true benefits of AI. The way I see it is that the ‘AI as an existential risk’ narrative is no different; where instead of the true benefits of AI, the focus is on exaggerating the risks of AI. The path forward is also not downplaying these risks. In my opinion, we need to focus our efforts on strong policy making and open discourse to facilitate responsible AI development. Clearly there is also a need for some regulatory oversight to address these risks. But a question needs to be asked:\n\nWhat do we want out of these regulations? Do we want to monopolize these developments in the hands of select few or do we truly want responsible AI systems?"
  }
]