<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aryamik Sharma">
<meta name="dcterms.date" content="2024-08-24">

<title>Aryamik Sharma - Is AI an ‘existential risk’?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aryamik Sharma</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">Sympatheia</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Disclaimer.html" rel="" target="">
 <span class="menu-text">Disclaimer</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aryamik-sharma" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:aryamik.sharma@uwaterloo.ca" rel="" target=""><i class="bi bi-envelope" role="img" aria-label="email">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Is AI an ‘existential risk’?</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://aryamik.github.io">Aryamik Sharma</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 24, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<p>From Terminator to Westworld, almost all depictions of AI that I have seen in media follow a similar theme: a malevolent entity that aims to subjugate human beings. In fact, I am having a hard time recalling any major piece of media that isn’t set in the dystopian future and depicts humans and AI living in harmony. If you have any good recommendations of a movie, TV series, novel or any other major piece of art that depicts humans and AI having a jolly good time, do let me know.</p>
<p>But coming to the concept of AI being an existential threat. How did I stumble across this idea?</p>
<p>The answer is this comprehensive <a href="https://80000hours.org/problem-profiles/artificial-intelligence/">article</a> published on 80000 hours. I would definitely recommend giving it a read because it outlines some really good points about the future of AI. In fact, risks from artificial intelligence are ranked as the number one in the world’s most pressing problems above catastrophic pandemics, nuclear weapons, power war and climate change. On March 22, 2023, thousands of tech leaders, researchers and AI experts including Elon Musk, Stuart Russell, Max Tegmark signed <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">an open letter</a> urging a moratorium on the development of the most powerful artificial intelligence systems. Almost two months later, a similar letter titled ‘<a href="https://www.safe.ai/work/statement-on-ai-risk#signatories">Statement on AI Risk</a>’ was signed by another group of world’s leading AI scientists and experts.</p>
<p>A natural question to ask would be: why is this all happening of a sudden?</p>
<p>The number of AI-lobbying organizations in the U.S. <a href="https://www.cnbc.com/2024/02/02/ai-lobbying-spikes-nearly-200percent-as-calls-for-regulation-surge.html">spiked from 158 in 2022 to 450 in 2023</a>. That marks a whopping 185% increase in AI lobbying with the goal of regulating AI. There have been reports suggesting that there is in fact a growing network of AI advisers, think tanks and policy groups that are <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">influencing federal agencies on AI policy discourse</a>. For example, a <a href="https://www.blumenthal.senate.gov/newsroom/press/release/blumenthal-and-hawley-announce-bipartisan-framework-on-artificial-intelligence-legislation">bipartisan bill</a> was passed in the U.S. Senate setting the framework for AI legislation. It would would create a new authority that any company developing AI would have to register to and seek a license from. Similar developments are taking place around the world. According to Stanford University’s <a href="https://aiindex.stanford.edu/ai-index-report-2023/">2023 AI Index</a>, the annual number of bills mentioning “artificial intelligence” passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.</p>
<p>At a first glance, all these developments look like a step in the right direction. In hands of bad actors, any technology like AI can be a bad thing. We are already seeing a rise in <a href="https://www.scientificamerican.com/article/a-safe-word-can-protect-against-ai-impostor-scams/">scams</a>, <a href="https://www.nature.com/articles/d41586-024-02521-3">deepfakes</a>, <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/the-terrifying-ai-scam-that-uses-your-loved-ones-voice">voice cloning</a>, <a href="https://www.theverge.com/2023/1/25/23571082/cnet-ai-written-stories-errors-corrections-red-ventures">misinformation</a> and <a href="https://www.thomsonreuters.com/en-us/posts/government/identity-theft-drivers/">cyberattacks</a>.</p>
<p>However, there are many who believe that this narrative of AI being an existential risk is being used to monopolize AI in hands of select few. Andrew Ng, a globally recognized leader in AI, believes that imposing strict licensing requirements and blanket AI regulations is is based on the <a href="https://www.afr.com/technology/google-brain-founder-says-big-tech-is-lying-about-ai-human-extinction-danger-20231027-p5efnz">“bad idea that AI could make us go extinct”</a>. <a href="https://venturebeat.com/ai/ai-pioneers-hinton-ng-lecun-bengio-amp-up-x-risk-debate/">In his view</a>, this is a way for big tech to create regulatory capture to ensure that open source alternatives <a href="https://www.businessinsider.com/andrew-ng-google-brain-big-tech-ai-risks-2023-10">cannot compete</a>. Regulatory capture is a concept where a regulatory agency enacts policies that favor the industry at the expense of the broader public interest, in this case with regulations that are too onerous or expensive for smaller businesses to meet. This has led to a growing movement to democratize AI governance. Letters such as <a href="https://open.mozilla.org/letter/">Joint Statement on AI Safety and Openness</a> and <a href="https://laion.ai/notes/letter-to-the-eu-parliament/">A Call to Protect and Open-Source AI in Europe</a> have been signed by various scientists, policymakers, researchers and other prominent figures in the field of AI.</p>
<p>Suresh Venkatasubramanian, a professor of computer science at Brown University who co-authored last year’s <a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">White House Blueprint for an AI Bill of Rights</a> believes that the focus on cataclysmic AI risk is <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">“speculative science fiction”</a> that borders on “fearmongering.” He argues:</p>
<blockquote class="blockquote">
<p>“There’s a push being made that the only thing we should care about is long-term risk because ‘It’s going to take over the world, Terminator, blah blah blah,’”</p>
<p>“I think it’s important to ask, what is the basis for these claims? What is the likelihood of these claims coming to pass? And how certain are we about all this?”</p>
</blockquote>
<p>When one sits down to analyse these claims, a natural question to start off would be:</p>
<p><em>‘Is superintelligence even possible?’</em> The answer to that question depends on who you ask. As Nick Bostrom mentions in Superintelligence, expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. According to <a href="https://intelligence.org/files/PredictingAI.pdf">Stuart Armstrong and Kaj Sotala</a>. predictions about the future development of artificial intelligence, “are as confident as they are diverse.”</p>
<p>Many surveys have been conducted on this topic where AI experts have been asked about their expectations for the future of machine intelligence (I really like this <a href="https://ourworldindata.org/ai-timelines">article</a> and this <a href="https://nickbostrom.com/papers/survey.pdf">survey</a> conducted by Nick Bostrom in 2012-2013) and turns out there is huge disagreement between experts about when human-level AI will be developed. Granted there are limitations when interpreting the results of these surveys, but the take home message is that there is a high degree of uncertainty about the future of AI. That does not mean there is no risk involved. But what is that risk exactly? Is it Skynet? Ultron? Or another killer robot?</p>
<p>In Life 3.0, Max Tegmark debunks the long standing narrative of AI going rogue:</p>
<blockquote class="blockquote">
<p>The fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants.</p>
<p>Therefore, <em>the real risk with AGI isn’t malice but competence</em>. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we’re in trouble.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><a href="https://youtu.be/8nt3edWLgIg"><img src="images/Screenshot from 2024-08-24 10-27-27.png" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption margin-caption">Source: Can we build AI without losing control over it? | Sam Harris</figcaption>
</figure>
</div>
<p>This raises the question: ’What do we do about these risks of AI and more importantly how do we make sure our goals align with that of a superintelligent AI? In the book, Max mentions that goal alignment remains to be an unsolved problem which would require its own post but broadly speaking it can be further subdivided into three sub-problems:</p>
<ol type="1">
<li><p>Making AI <em>learn</em> our goals</p></li>
<li><p>Making AI <em>adopt</em> our goals</p></li>
<li><p>Making AI <em>retain</em> our goals</p></li>
</ol>
<p>To wrap things up, any technology including AI poses a risk. This risk stems from misalignment between our goals and the goal of AI. However, we also need to be careful that we are not overselling these risks. In my last post, I talked about how ‘AI Washing’ stems from overselling the true benefits of AI. The way I see it is that the ‘AI as an existential risk’ narrative is no different; where instead of the true benefits of AI, the focus is on exaggerating the risks of AI. The path forward is also not downplaying these risks. In my opinion, we need to focus our efforts on strong policy making and open discourse to facilitate responsible AI development. Clearly there is also a need for <em>some</em> regulatory oversight to address these risks. But a question needs to be asked:</p>
<blockquote class="blockquote">
<p>What do we want out of these regulations? Do we want to monopolize these developments in the hands of select few or do we truly want responsible AI systems?</p>
</blockquote>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="Aryamik/aryamik.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>