{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Building an ESG Analyst using Llama\"\n",
        "#description: \"blog post description (appears underneath the title in smaller text) which is included on the listing page\"\n",
        "author:\n",
        "  - name: Aryamik Sharma\n",
        "    url: https://aryamik.github.io\n",
        "\n",
        "date: 12-29-2023\n",
        "categories: [Data Analytics] # self-defined categories\n",
        "\n",
        "draft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n",
        "---\n",
        "\n",
        "\n",
        "Last year, I finally managed to cross an item of my bucketlist\n",
        "\n",
        "-   Assemble my own Gaming PC ‚úÖ\n",
        "\n",
        "Look at this beauty -\n",
        "\n",
        "Upon setting it up, I did what any tech nerd would do.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "<p align = 'center'><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1DyD2jrWjFM?si=ap_D5Y-yvTsCNV2a\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></p>\n",
        "```\n",
        "\n",
        "\n",
        "All this horsepower and no room to gallop\n",
        "\n",
        "So eventually, I was like \"you know what, now that I have all this horsepower, why don't I try my hands at Retrieval Augmented Generation (RAG) as it is the latest talk of the town?\"\n",
        "\n",
        "It just so happened that I stumbled upon work by Markus Leippold and his team at the University of Zurich on using Large Language Models (LLMs) for analysing sustainability reports. I discovered the tutorials Tobias Schimanski has published some good tutorials on doing this.\n",
        "\n",
        "As I was in the process of implementing the solution, I noticed that the code was pretty up to what I was looking for. However, as I was going through the code, I thought of couple of areas that I could play around with\n",
        "\n",
        "-   For starters, Tobias uses OpenAI. While I could create my own OpenAI API keys, I wanted to use an open source model, something like Llama.\n",
        "\n",
        "-   For parsing PDFs, the tutorial uses PyPDF. While it does an amazing job of parsing documents, for more complex documents like sustainability reports that are not standardized. using Llamaparser, would result in better outputs. I came across this idea in [this](https://www.youtube.com/watch?v=u5Vcrwpzoz8) tutorial by AI Jason.\n",
        "\n",
        "The tutorial provide a good use case of using RAG to analyse sustainability reports. More specifically, extracting basic information about a company, its sector etc. I asked myself - \"Why not take this one step further? Why not create an AI agent that helps me analyse a sustainability report and see whether it meets the principles outlined in Canada's Competition Act, specifically around environmental claims and greenwashing.\n",
        "\n",
        "Last year, the Competition Bureau released its draft principles on environmental claims. While they have not been finalized at the time of this writing, I thought it would be a good starting good point for this use case.\n",
        "\n",
        "From an analytics perspective, greenwashing as a topic has always fascinated me. Why, you may ask?\n",
        "\n",
        "Simply put, it's because it is hard to 'quantify' greenwashing. Sure, it does involve numbers. Let's say I go out on a limb and say that my product is going to save 50% emissions compared to other products in the market, you would be like 'How did you come up with that number?' and I 'm going to be like:\n",
        "\n",
        "![](images/proxy-image.jpg){width=\"422\"}\n",
        "\n",
        "You would tell me to do a trend analysis of my historical emissions or by first sourcing the emissions data of the industry, sourcing data of other products. Maybe even do a Life cycle assessment (LCA) and then I might get to a number. So yes, there are numbers involved. But it's not as if I can simply 'measure' greenwashing. Over the last few months, I was curious to see if there where was any work done on this topic. I came across some good papers such as [this](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4582917) one by Lagasio (2023) who introduces 'Greenwashing Severity Index' (GSI) or [this](https://doi.org/10.1007/s10551-012-1360-0)one by Chen and Chang (2013) who suggest measuring greenwashing on product level, green consumer confusion, green perceived risk and green trust based on survey responses.\n",
        "\n",
        "However, [Dorfleitner and Utz (2023)](https://link.springer.com/article/10.1007/s11846-023-00718-w) sum it up best :\n",
        "\n",
        "> So far, the literature does not present a widely accepted framework to\n",
        "> measure greenwashing. Moreover, approaches based on surveys and case\n",
        "> studies are not scalable on a broad sample of firms.\n",
        "\n",
        "In short, there is no unified framework to measure greenwashing. So while this use case is not an attempt to quantify greenwashing, it's simply a glimpse into how we could leverage analytics to make our lives easier.\n"
      ],
      "id": "c8ec3e88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# --- Prompt Template: GENERAL INFO ---\n",
        "PROMPT_TEMPLATE_GENERAL = \"\"\"\n",
        "You are tasked with the role of a climate scientist, assigned to analyze a company's sustainability report. Based on the following extracted parts from the sustainability report, answer the given QUESTIONS.\n",
        "If you don't know the answer, just say that you don't know by answering \"NA\". Don't try to make up an answer.\n",
        "\n",
        "Given are the following sources:\n",
        "--------------------- [BEGIN OF SOURCES]\n",
        "{context}\n",
        "--------------------- [END OF SOURCES]\n",
        "\n",
        "QUESTIONS:\n",
        "1. What is the company of the report?\n",
        "2. What sector does the company belong to?\n",
        "3. Where is the company located?\n",
        "\n",
        "Format your answers in JSON format with the following keys: COMPANY_NAME, COMPANY_SECTOR, COMPANY_LOCATION.\n",
        "Your FINAL_ANSWER in JSON (ensure there's no format error):\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- Load and parse document ---\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-8zFsxaLmD47ZZ4bqBysEEZ9I9E0dNgDL7jrwDPGotjTXdNnQ\"  # Replace with your actual key\n",
        "parser = LlamaParse(result_type=\"markdown\")\n",
        "file_extractor = {\".pdf\": parser}\n",
        "\n",
        "docs = SimpleDirectoryReader(\n",
        "    input_files=[r\"C:\\Users\\aryam\\Downloads\\Documents\\427857-1-_7_Nike-2024-Combo_Form-10-K_WR.pdf\"],\n",
        "    file_extractor=file_extractor\n",
        ").load_data()\n",
        "\n",
        "# --- Split into chunks ---\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "document_splits = []\n",
        "\n",
        "for doc in docs:\n",
        "    chunks = text_splitter.split_text(doc.text)\n",
        "    for chunk in chunks:\n",
        "        document_splits.append(Document(page_content=chunk, metadata={\"source\": doc.get_doc_id()}))\n",
        "\n",
        "# --- Embed and store ---\n",
        "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "vectorstore = Chroma.from_documents(documents=document_splits, embedding=local_embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# --- Load LLM model ---\n",
        "model = ChatOllama(model=\"gemma3:27b\")  # Ensure this model is available in Ollama\n",
        "\n",
        "# --- Build the RAG chain using the GENERAL template ---\n",
        "general_prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_GENERAL)\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(model, general_prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "# --- Run the pipeline ---\n",
        "# --- Run the pipeline ---\n",
        "results = rag_chain.invoke({\"input\": \"Extract basic company information from the report.\"})\n",
        "\n",
        "# --- Parse and pretty-print the answer ---\n",
        "try:\n",
        "    final_answer = json.loads(results[\"answer\"])\n",
        "    print(\"\\nüìÑ Extracted Company Information:\")\n",
        "    print(f\"üîπ Company Name   : {final_answer.get('COMPANY_NAME', 'NA')}\")\n",
        "    print(f\"üîπ Sector         : {final_answer.get('COMPANY_SECTOR', 'NA')}\")\n",
        "    print(f\"üîπ Location       : {final_answer.get('COMPANY_LOCATION', 'NA')}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"\\n‚ö†Ô∏è Unable to parse model output as JSON:\")\n",
        "    print(results[\"answer\"])"
      ],
      "id": "efa46560",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So there you have it. A very simple use case of RAG to analyse greenwashing. Is it perfect? No, far from it. At the end of the day, it still requires human overlay because like I said earlier, greenwashing is a very nuanced topic. Nevertheless, it is still pretty cool to play around with it and get a sense of what is possible within the realm of ESG and AI.\n",
        "\n",
        "As far as next steps are concerned, I will try to tinker around with it.\n",
        "\n",
        "Overall, this was a fun learning project for me. As a newbie to RAG, I have barely scratched the surface. There are so many rabbit holes I want to explore such as playing around with different models, refining the prompts and tinkering with other technical parameters to get better outputs.\n",
        "\n",
        "Maybe also create a nice web application. Some of the ones that I found really interesting were\n",
        "\n",
        "-   [SEC Insights](https://github.com/run-llama/sec-insights?tab=readme-ov-file) - As I was exploring some of the different use cases of RAG, I found that a 10-K RAG agent was very popular. You have an agent that scour through complex 10-K filings and easily answer some of the questions you might have. I really liked this one because it largely overlaps with my use case. Granted 10-K filings have a structured format, I thought analysing ESG reports seems like a natural extension of these financial chatbots.\n",
        "\n",
        "-   Llama Banker - Similar to the previous one, but I stumbled across this tutorial and really liked how Nick highlighted some of the challenges he encountered while building this application.\n",
        "\n",
        "-   [Agentic Company Researcher](https://github.com/pogjester/company-research-agent) - This one is a little bit more advanced as it involves retrieving data from multiple sources such as company websites, news articles, financial reports, and industry analyses.\n",
        "\n",
        "why not expand it to ESG and Sustainable Finance.\n",
        "\n",
        "mpany Researcher\n",
        "\n",
        "# Competition Act\n",
        "\n",
        "The basic\n",
        "\n",
        "scan the documents, I will provide instructions . For referemce ,\n",
        "\n",
        "The only thing is it was using OpenAI. Llama was the key, I wanted to build something that was open-source.\n",
        "\n",
        "My first reaction was -Llama Models.\n",
        "\n",
        "<https://www.youtube.com/watch?v=u5Vcrwpzoz8>"
      ],
      "id": "0efec104"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\aryam\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}